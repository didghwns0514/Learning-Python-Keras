{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regression 회귀 문제\n",
    "## logistic regression - 분류 문제 이것과는 다름\n",
    "- 참조 링크 : [link](https://tensorflow.blog/%EC%BC%80%EB%9D%BC%EC%8A%A4-%EB%94%A5%EB%9F%AC%EB%8B%9D/3-6-%EC%A3%BC%ED%83%9D-%EA%B0%80%EA%B2%A9-%EC%98%88%EC%B8%A1-%ED%9A%8C%EA%B7%80-%EB%AC%B8%EC%A0%9C/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, ..., 2.10000e+01,\n",
       "        3.96900e+02, 1.87200e+01],\n",
       "       [2.17700e-02, 8.25000e+01, 2.03000e+00, ..., 1.47000e+01,\n",
       "        3.95380e+02, 3.11000e+00],\n",
       "       [4.89822e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        3.75520e+02, 3.26000e+00],\n",
       "       ...,\n",
       "       [3.46600e-02, 3.50000e+01, 6.06000e+00, ..., 1.69000e+01,\n",
       "        3.62250e+02, 7.83000e+00],\n",
       "       [2.14918e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        2.61950e+02, 1.57900e+01],\n",
       "       [1.43900e-02, 6.00000e+01, 2.93000e+00, ..., 1.56000e+01,\n",
       "        3.76700e+02, 4.38000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
       "       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
       "       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
       "       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
       "       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
       "       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
       "       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
       "       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
       "       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
       "       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
       "       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
       "       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
       "       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
       "       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
       "       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
       "       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
       "        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
       "       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
       "       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
       "       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
       "       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
       "       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
       "       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
       "       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
       "       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
       "       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
       "        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
       "        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
       "       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
       "       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
       "       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
       "       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
       "       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
       "       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
       "       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
       "       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
       "       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1) 데이터 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean : [3.74511057e+00 1.14801980e+01 1.11044307e+01 6.18811881e-02\n",
      " 5.57355941e-01 6.26708168e+00 6.90106436e+01 3.74027079e+00\n",
      " 9.44059406e+00 4.05898515e+02 1.84759901e+01 3.54783168e+02\n",
      " 1.27408168e+01]\n",
      "std : [9.22929073e+00 2.37382770e+01 6.80287253e+00 2.40939633e-01\n",
      " 1.17147847e-01 7.08908627e-01 2.79060634e+01 2.02770050e+00\n",
      " 8.68758849e+00 1.66168506e+02 2.19765689e+00 9.39946015e+01\n",
      " 7.24556085e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.27224633, -0.48361547, -0.43576161, ...,  1.14850044,\n",
       "         0.44807713,  0.8252202 ],\n",
       "       [-0.40342651,  2.99178419, -1.33391162, ..., -1.71818909,\n",
       "         0.43190599, -1.32920239],\n",
       "       [ 0.1249402 , -0.48361547,  1.0283258 , ...,  0.78447637,\n",
       "         0.22061726, -1.30850006],\n",
       "       ...,\n",
       "       [-0.40202987,  0.99079651, -0.7415148 , ..., -0.71712291,\n",
       "         0.07943894, -0.67776904],\n",
       "       [-0.17292018, -0.48361547,  1.24588095, ..., -1.71818909,\n",
       "        -0.98764362,  0.42083466],\n",
       "       [-0.40422614,  2.04394792, -1.20161456, ..., -1.30866202,\n",
       "         0.23317118, -1.15392266]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 데이터 미포함한 데이터로 정규화 들어가야 함\n",
    "mean = X_train.mean(axis=0)\n",
    "X_train -= mean\n",
    "std = X_train.std(axis=0)\n",
    "X_train /= std\n",
    "\n",
    "X_test -= mean\n",
    "X_test /= std\n",
    "\n",
    "print(f'mean : {mean}')\n",
    "print(f'std : {std}')\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2) Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(404,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Y_train))\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(X_test))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(102,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Y_test))\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                896       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,121\n",
      "Trainable params: 5,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "#model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],) ))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method compile in module keras.engine.training:\n",
      "\n",
      "compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs) method of keras.engine.sequential.Sequential instance\n",
      "    Configures the model for training.\n",
      "    \n",
      "    # Arguments\n",
      "        optimizer: String (name of optimizer) or optimizer instance.\n",
      "            See [optimizers](/optimizers).\n",
      "        loss: String (name of objective function) or objective function.\n",
      "            See [losses](/losses).\n",
      "            If the model has multiple outputs, you can use a different loss\n",
      "            on each output by passing a dictionary or a list of losses.\n",
      "            The loss value that will be minimized by the model\n",
      "            will then be the sum of all individual losses.\n",
      "        metrics: List of metrics to be evaluated by the model\n",
      "            during training and testing.\n",
      "            Typically you will use `metrics=['accuracy']`.\n",
      "            To specify different metrics for different outputs of a\n",
      "            multi-output model, you could also pass a dictionary,\n",
      "            such as `metrics={'output_a': 'accuracy'}`.\n",
      "        loss_weights: Optional list or dictionary specifying scalar\n",
      "            coefficients (Python floats) to weight the loss contributions\n",
      "            of different model outputs.\n",
      "            The loss value that will be minimized by the model\n",
      "            will then be the *weighted sum* of all individual losses,\n",
      "            weighted by the `loss_weights` coefficients.\n",
      "            If a list, it is expected to have a 1:1 mapping\n",
      "            to the model's outputs. If a tensor, it is expected to map\n",
      "            output names (strings) to scalar coefficients.\n",
      "        sample_weight_mode: If you need to do timestep-wise\n",
      "            sample weighting (2D weights), set this to `\"temporal\"`.\n",
      "            `None` defaults to sample-wise weights (1D).\n",
      "            If the model has multiple outputs, you can use a different\n",
      "            `sample_weight_mode` on each output by passing a\n",
      "            dictionary or a list of modes.\n",
      "        weighted_metrics: List of metrics to be evaluated and weighted\n",
      "            by sample_weight or class_weight during training and testing.\n",
      "        target_tensors: By default, Keras will create placeholders for the\n",
      "            model's target, which will be fed with the target data during\n",
      "            training. If instead you would like to use your own\n",
      "            target tensors (in turn, Keras will not expect external\n",
      "            Numpy data for these targets at training time), you\n",
      "            can specify them via the `target_tensors` argument. It can be\n",
      "            a single tensor (for a single-output model), a list of tensors,\n",
      "            or a dict mapping output names to target tensors.\n",
      "        **kwargs: When using the Theano/CNTK backends, these arguments\n",
      "            are passed into `K.function`.\n",
      "            When using the TensorFlow backend,\n",
      "            these arguments are passed into `tf.Session.run`.\n",
      "    \n",
      "    # Raises\n",
      "        ValueError: In case of invalid arguments for\n",
      "            `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1cb3331b668>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3) train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EarlyStopping in module keras.callbacks:\n",
      "\n",
      "class EarlyStopping(Callback)\n",
      " |  EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |  \n",
      " |  Stop training when a monitored quantity has stopped improving.\n",
      " |  \n",
      " |  # Arguments\n",
      " |      monitor: quantity to be monitored.\n",
      " |      min_delta: minimum change in the monitored quantity\n",
      " |          to qualify as an improvement, i.e. an absolute\n",
      " |          change of less than min_delta, will count as no\n",
      " |          improvement.\n",
      " |      patience: number of epochs with no improvement\n",
      " |          after which training will be stopped.\n",
      " |      verbose: verbosity mode.\n",
      " |      mode: one of {auto, min, max}. In `min` mode,\n",
      " |          training will stop when the quantity\n",
      " |          monitored has stopped decreasing; in `max`\n",
      " |          mode it will stop when the quantity\n",
      " |          monitored has stopped increasing; in `auto`\n",
      " |          mode, the direction is automatically inferred\n",
      " |          from the name of the monitored quantity.\n",
      " |      baseline: Baseline value for the monitored quantity to reach.\n",
      " |          Training will stop if the model doesn't show improvement\n",
      " |          over the baseline.\n",
      " |      restore_best_weights: whether to restore model weights from\n",
      " |          the epoch with the best value of the monitored quantity.\n",
      " |          If False, the model weights obtained at the last step of\n",
      " |          training are used.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EarlyStopping\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_monitor_value(self, logs)\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ModelCheckpoint in module keras.callbacks:\n",
      "\n",
      "class ModelCheckpoint(Callback)\n",
      " |  ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
      " |  \n",
      " |  Save the model after every epoch.\n",
      " |  \n",
      " |  `filepath` can contain named formatting options,\n",
      " |  which will be filled the value of `epoch` and\n",
      " |  keys in `logs` (passed in `on_epoch_end`).\n",
      " |  \n",
      " |  For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n",
      " |  then the model checkpoints will be saved with the epoch number and\n",
      " |  the validation loss in the filename.\n",
      " |  \n",
      " |  # Arguments\n",
      " |      filepath: string, path to save the model file.\n",
      " |      monitor: quantity to monitor.\n",
      " |      verbose: verbosity mode, 0 or 1.\n",
      " |      save_best_only: if `save_best_only=True`,\n",
      " |          the latest best model according to\n",
      " |          the quantity monitored will not be overwritten.\n",
      " |      mode: one of {auto, min, max}.\n",
      " |          If `save_best_only=True`, the decision\n",
      " |          to overwrite the current save file is made\n",
      " |          based on either the maximization or the\n",
      " |          minimization of the monitored quantity. For `val_acc`,\n",
      " |          this should be `max`, for `val_loss` this should\n",
      " |          be `min`, etc. In `auto` mode, the direction is\n",
      " |          automatically inferred from the name of the monitored quantity.\n",
      " |      save_weights_only: if True, then only the model's weights will be\n",
      " |          saved (`model.save_weights(filepath)`), else the full model\n",
      " |          is saved (`model.save(filepath)`).\n",
      " |      period: Interval (number of epochs) between checkpoints.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ModelCheckpoint\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=200, mode='min', monitor='val_loss')\n",
    "mc = ModelCheckpoint(filepath='best_model.h5', verbose=1, save_best_only=True, mode='min', monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module keras.engine.training:\n",
      "\n",
      "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs) method of keras.engine.sequential.Sequential instance\n",
      "    Trains the model for a given number of epochs (iterations on a dataset).\n",
      "    \n",
      "    # Arguments\n",
      "        x: Numpy array of training data (if the model has a single input),\n",
      "            or list of Numpy arrays (if the model has multiple inputs).\n",
      "            If input layers in the model are named, you can also pass a\n",
      "            dictionary mapping input names to Numpy arrays.\n",
      "            `x` can be `None` (default) if feeding from\n",
      "            framework-native tensors (e.g. TensorFlow data tensors).\n",
      "        y: Numpy array of target (label) data\n",
      "            (if the model has a single output),\n",
      "            or list of Numpy arrays (if the model has multiple outputs).\n",
      "            If output layers in the model are named, you can also pass a\n",
      "            dictionary mapping output names to Numpy arrays.\n",
      "            `y` can be `None` (default) if feeding from\n",
      "            framework-native tensors (e.g. TensorFlow data tensors).\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See [callbacks](/callbacks).\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate\n",
      "            the loss and any model metrics\n",
      "            on this data at the end of each epoch.\n",
      "            The validation data is selected from the last samples\n",
      "            in the `x` and `y` data provided, before shuffling.\n",
      "        validation_data: tuple `(x_val, y_val)` or tuple\n",
      "            `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data.\n",
      "            `validation_data` will override `validation_split`.\n",
      "        shuffle: Boolean (whether to shuffle the training data\n",
      "            before each epoch) or str (for 'batch').\n",
      "            'batch' is a special option for dealing with the\n",
      "            limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      "            Has no effect when `steps_per_epoch` is not `None`.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample.\n",
      "            In this case you should make sure to specify\n",
      "            `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Integer or `None`.\n",
      "            Total number of steps (batches of samples)\n",
      "            before declaring one epoch finished and starting the\n",
      "            next epoch. When training with input tensors such as\n",
      "            TensorFlow data tensors, the default `None` is equal to\n",
      "            the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined.\n",
      "        validation_steps: Only relevant if `steps_per_epoch`\n",
      "            is specified. Total number of steps (batches of samples)\n",
      "            to validate before stopping.\n",
      "    \n",
      "    # Returns\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    # Raises\n",
      "        RuntimeError: If the model was never compiled.\n",
      "        ValueError: In case of mismatch between the provided input data\n",
      "            and what the model expects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 355 samples, validate on 49 samples\n",
      "Epoch 1/3000\n",
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\82102\\Anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "355/355 [==============================] - 1s 3ms/step - loss: 576.8886 - mean_absolute_error: 22.0415 - val_loss: 481.5005 - val_mean_absolute_error: 20.7321\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 481.50051, saving model to best_model.h5\n",
      "Epoch 2/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 528.0253 - mean_absolute_error: 20.8566 - val_loss: 433.8856 - val_mean_absolute_error: 19.4456\n",
      "\n",
      "Epoch 00002: val_loss improved from 481.50051 to 433.88563, saving model to best_model.h5\n",
      "Epoch 3/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 468.2571 - mean_absolute_error: 19.3362 - val_loss: 372.8378 - val_mean_absolute_error: 17.8027\n",
      "\n",
      "Epoch 00003: val_loss improved from 433.88563 to 372.83779, saving model to best_model.h5\n",
      "Epoch 4/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 391.8539 - mean_absolute_error: 17.3074 - val_loss: 292.3673 - val_mean_absolute_error: 15.4490\n",
      "\n",
      "Epoch 00004: val_loss improved from 372.83779 to 292.36726, saving model to best_model.h5\n",
      "Epoch 5/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 295.0964 - mean_absolute_error: 14.5696 - val_loss: 197.3672 - val_mean_absolute_error: 12.3917\n",
      "\n",
      "Epoch 00005: val_loss improved from 292.36726 to 197.36720, saving model to best_model.h5\n",
      "Epoch 6/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 189.6070 - mean_absolute_error: 11.2100 - val_loss: 111.7283 - val_mean_absolute_error: 8.6868\n",
      "\n",
      "Epoch 00006: val_loss improved from 197.36720 to 111.72832, saving model to best_model.h5\n",
      "Epoch 7/3000\n",
      "355/355 [==============================] - 0s 87us/step - loss: 110.5034 - mean_absolute_error: 8.1836 - val_loss: 64.1561 - val_mean_absolute_error: 6.5524\n",
      "\n",
      "Epoch 00007: val_loss improved from 111.72832 to 64.15606, saving model to best_model.h5\n",
      "Epoch 8/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 73.6091 - mean_absolute_error: 6.5213 - val_loss: 49.9618 - val_mean_absolute_error: 5.8255\n",
      "\n",
      "Epoch 00008: val_loss improved from 64.15606 to 49.96184, saving model to best_model.h5\n",
      "Epoch 9/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 57.6663 - mean_absolute_error: 5.6968 - val_loss: 38.0524 - val_mean_absolute_error: 5.1183\n",
      "\n",
      "Epoch 00009: val_loss improved from 49.96184 to 38.05242, saving model to best_model.h5\n",
      "Epoch 10/3000\n",
      "355/355 [==============================] - 0s 85us/step - loss: 46.1172 - mean_absolute_error: 5.0380 - val_loss: 30.7691 - val_mean_absolute_error: 4.6661\n",
      "\n",
      "Epoch 00010: val_loss improved from 38.05242 to 30.76912, saving model to best_model.h5\n",
      "Epoch 11/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 38.3098 - mean_absolute_error: 4.5288 - val_loss: 25.5386 - val_mean_absolute_error: 4.2742\n",
      "\n",
      "Epoch 00011: val_loss improved from 30.76912 to 25.53858, saving model to best_model.h5\n",
      "Epoch 12/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 32.7147 - mean_absolute_error: 4.1167 - val_loss: 22.1307 - val_mean_absolute_error: 3.9971\n",
      "\n",
      "Epoch 00012: val_loss improved from 25.53858 to 22.13075, saving model to best_model.h5\n",
      "Epoch 13/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 29.1149 - mean_absolute_error: 3.8385 - val_loss: 20.3337 - val_mean_absolute_error: 3.8617\n",
      "\n",
      "Epoch 00013: val_loss improved from 22.13075 to 20.33365, saving model to best_model.h5\n",
      "Epoch 14/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 26.3121 - mean_absolute_error: 3.6417 - val_loss: 18.6040 - val_mean_absolute_error: 3.6874\n",
      "\n",
      "Epoch 00014: val_loss improved from 20.33365 to 18.60400, saving model to best_model.h5\n",
      "Epoch 15/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 24.3969 - mean_absolute_error: 3.5221 - val_loss: 16.5755 - val_mean_absolute_error: 3.4588\n",
      "\n",
      "Epoch 00015: val_loss improved from 18.60400 to 16.57552, saving model to best_model.h5\n",
      "Epoch 16/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 22.7822 - mean_absolute_error: 3.4071 - val_loss: 16.5585 - val_mean_absolute_error: 3.4399\n",
      "\n",
      "Epoch 00016: val_loss improved from 16.57552 to 16.55855, saving model to best_model.h5\n",
      "Epoch 17/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 21.2908 - mean_absolute_error: 3.3155 - val_loss: 17.0802 - val_mean_absolute_error: 3.4919\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 16.55855\n",
      "Epoch 18/3000\n",
      "355/355 [==============================] - 0s 93us/step - loss: 20.2978 - mean_absolute_error: 3.2214 - val_loss: 15.7987 - val_mean_absolute_error: 3.3070\n",
      "\n",
      "Epoch 00018: val_loss improved from 16.55855 to 15.79870, saving model to best_model.h5\n",
      "Epoch 19/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 19.3342 - mean_absolute_error: 3.1127 - val_loss: 14.7315 - val_mean_absolute_error: 3.1657\n",
      "\n",
      "Epoch 00019: val_loss improved from 15.79870 to 14.73155, saving model to best_model.h5\n",
      "Epoch 20/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 18.5371 - mean_absolute_error: 3.0385 - val_loss: 14.4472 - val_mean_absolute_error: 3.1152\n",
      "\n",
      "Epoch 00020: val_loss improved from 14.73155 to 14.44716, saving model to best_model.h5\n",
      "Epoch 21/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 17.8211 - mean_absolute_error: 3.0092 - val_loss: 14.5338 - val_mean_absolute_error: 3.1196\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 14.44716\n",
      "Epoch 22/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 17.3654 - mean_absolute_error: 2.9945 - val_loss: 14.4677 - val_mean_absolute_error: 3.0966\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 14.44716\n",
      "Epoch 23/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 16.8907 - mean_absolute_error: 3.0127 - val_loss: 14.8651 - val_mean_absolute_error: 3.1214\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 14.44716\n",
      "Epoch 24/3000\n",
      "355/355 [==============================] - 0s 70us/step - loss: 16.5521 - mean_absolute_error: 2.9716 - val_loss: 13.6953 - val_mean_absolute_error: 2.9512\n",
      "\n",
      "Epoch 00024: val_loss improved from 14.44716 to 13.69532, saving model to best_model.h5\n",
      "Epoch 25/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 15.8642 - mean_absolute_error: 2.8430 - val_loss: 13.0853 - val_mean_absolute_error: 2.9326\n",
      "\n",
      "Epoch 00025: val_loss improved from 13.69532 to 13.08535, saving model to best_model.h5\n",
      "Epoch 26/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 79us/step - loss: 15.4317 - mean_absolute_error: 2.7946 - val_loss: 13.0394 - val_mean_absolute_error: 2.9362\n",
      "\n",
      "Epoch 00026: val_loss improved from 13.08535 to 13.03939, saving model to best_model.h5\n",
      "Epoch 27/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 15.0789 - mean_absolute_error: 2.7646 - val_loss: 12.5546 - val_mean_absolute_error: 2.8670\n",
      "\n",
      "Epoch 00027: val_loss improved from 13.03939 to 12.55461, saving model to best_model.h5\n",
      "Epoch 28/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 14.7022 - mean_absolute_error: 2.7266 - val_loss: 11.7783 - val_mean_absolute_error: 2.7232\n",
      "\n",
      "Epoch 00028: val_loss improved from 12.55461 to 11.77833, saving model to best_model.h5\n",
      "Epoch 29/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 14.5162 - mean_absolute_error: 2.7340 - val_loss: 12.0075 - val_mean_absolute_error: 2.7564\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 11.77833\n",
      "Epoch 30/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 14.2537 - mean_absolute_error: 2.6749 - val_loss: 11.1689 - val_mean_absolute_error: 2.6288\n",
      "\n",
      "Epoch 00030: val_loss improved from 11.77833 to 11.16890, saving model to best_model.h5\n",
      "Epoch 31/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 14.0066 - mean_absolute_error: 2.7006 - val_loss: 11.9530 - val_mean_absolute_error: 2.7434\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 11.16890\n",
      "Epoch 32/3000\n",
      "355/355 [==============================] - ETA: 0s - loss: 32.5618 - mean_absolute_error: 3.50 - 0s 70us/step - loss: 13.8457 - mean_absolute_error: 2.6931 - val_loss: 11.1233 - val_mean_absolute_error: 2.6427\n",
      "\n",
      "Epoch 00032: val_loss improved from 11.16890 to 11.12327, saving model to best_model.h5\n",
      "Epoch 33/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 13.5205 - mean_absolute_error: 2.6599 - val_loss: 11.4958 - val_mean_absolute_error: 2.6970\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 11.12327\n",
      "Epoch 34/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 13.4564 - mean_absolute_error: 2.6939 - val_loss: 12.0410 - val_mean_absolute_error: 2.7049\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 11.12327\n",
      "Epoch 35/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 13.2450 - mean_absolute_error: 2.6498 - val_loss: 11.7455 - val_mean_absolute_error: 2.7612\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 11.12327\n",
      "Epoch 36/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 12.5385 - mean_absolute_error: 2.5313 - val_loss: 10.5312 - val_mean_absolute_error: 2.6313\n",
      "\n",
      "Epoch 00036: val_loss improved from 11.12327 to 10.53121, saving model to best_model.h5\n",
      "Epoch 37/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 12.3159 - mean_absolute_error: 2.4913 - val_loss: 10.1255 - val_mean_absolute_error: 2.5378\n",
      "\n",
      "Epoch 00037: val_loss improved from 10.53121 to 10.12552, saving model to best_model.h5\n",
      "Epoch 38/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 12.1231 - mean_absolute_error: 2.4826 - val_loss: 10.2491 - val_mean_absolute_error: 2.5913\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 10.12552\n",
      "Epoch 39/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 11.9732 - mean_absolute_error: 2.4633 - val_loss: 9.9266 - val_mean_absolute_error: 2.5378\n",
      "\n",
      "Epoch 00039: val_loss improved from 10.12552 to 9.92659, saving model to best_model.h5\n",
      "Epoch 40/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 11.7431 - mean_absolute_error: 2.4384 - val_loss: 9.4433 - val_mean_absolute_error: 2.4793\n",
      "\n",
      "Epoch 00040: val_loss improved from 9.92659 to 9.44332, saving model to best_model.h5\n",
      "Epoch 41/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 11.6454 - mean_absolute_error: 2.4300 - val_loss: 8.8444 - val_mean_absolute_error: 2.3830\n",
      "\n",
      "Epoch 00041: val_loss improved from 9.44332 to 8.84444, saving model to best_model.h5\n",
      "Epoch 42/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 11.5157 - mean_absolute_error: 2.3835 - val_loss: 9.2051 - val_mean_absolute_error: 2.5024\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 8.84444\n",
      "Epoch 43/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 11.2735 - mean_absolute_error: 2.3570 - val_loss: 9.0509 - val_mean_absolute_error: 2.4611\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 8.84444\n",
      "Epoch 44/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 11.2439 - mean_absolute_error: 2.3648 - val_loss: 8.7813 - val_mean_absolute_error: 2.4076\n",
      "\n",
      "Epoch 00044: val_loss improved from 8.84444 to 8.78126, saving model to best_model.h5\n",
      "Epoch 45/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 11.0842 - mean_absolute_error: 2.3493 - val_loss: 8.4593 - val_mean_absolute_error: 2.3494\n",
      "\n",
      "Epoch 00045: val_loss improved from 8.78126 to 8.45930, saving model to best_model.h5\n",
      "Epoch 46/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 11.0843 - mean_absolute_error: 2.3318 - val_loss: 8.8595 - val_mean_absolute_error: 2.4608\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 8.45930\n",
      "Epoch 47/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 11.0339 - mean_absolute_error: 2.3219 - val_loss: 8.6067 - val_mean_absolute_error: 2.3798\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 8.45930\n",
      "Epoch 48/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 10.7397 - mean_absolute_error: 2.3023 - val_loss: 8.1519 - val_mean_absolute_error: 2.3203\n",
      "\n",
      "Epoch 00048: val_loss improved from 8.45930 to 8.15186, saving model to best_model.h5\n",
      "Epoch 49/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 10.5828 - mean_absolute_error: 2.2840 - val_loss: 8.1239 - val_mean_absolute_error: 2.3324\n",
      "\n",
      "Epoch 00049: val_loss improved from 8.15186 to 8.12388, saving model to best_model.h5\n",
      "Epoch 50/3000\n",
      "355/355 [==============================] - 0s 92us/step - loss: 10.5498 - mean_absolute_error: 2.2819 - val_loss: 8.3266 - val_mean_absolute_error: 2.3748\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 8.12388\n",
      "Epoch 51/3000\n",
      "355/355 [==============================] - 0s 87us/step - loss: 10.4272 - mean_absolute_error: 2.2557 - val_loss: 7.8697 - val_mean_absolute_error: 2.2927\n",
      "\n",
      "Epoch 00051: val_loss improved from 8.12388 to 7.86973, saving model to best_model.h5\n",
      "Epoch 52/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 10.4173 - mean_absolute_error: 2.2650 - val_loss: 7.8730 - val_mean_absolute_error: 2.2920\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 7.86973\n",
      "Epoch 53/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 10.2073 - mean_absolute_error: 2.2424 - val_loss: 7.8276 - val_mean_absolute_error: 2.2915\n",
      "\n",
      "Epoch 00053: val_loss improved from 7.86973 to 7.82755, saving model to best_model.h5\n",
      "Epoch 54/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 10.2029 - mean_absolute_error: 2.2472 - val_loss: 8.0910 - val_mean_absolute_error: 2.3342\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 7.82755\n",
      "Epoch 55/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 10.1537 - mean_absolute_error: 2.2425 - val_loss: 7.6148 - val_mean_absolute_error: 2.2514\n",
      "\n",
      "Epoch 00055: val_loss improved from 7.82755 to 7.61481, saving model to best_model.h5\n",
      "Epoch 56/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 10.1749 - mean_absolute_error: 2.2602 - val_loss: 7.5243 - val_mean_absolute_error: 2.2350\n",
      "\n",
      "Epoch 00056: val_loss improved from 7.61481 to 7.52429, saving model to best_model.h5\n",
      "Epoch 57/3000\n",
      "355/355 [==============================] - 0s 85us/step - loss: 9.8954 - mean_absolute_error: 2.2169 - val_loss: 7.6014 - val_mean_absolute_error: 2.2742\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 7.52429\n",
      "Epoch 58/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 9.8184 - mean_absolute_error: 2.1854 - val_loss: 7.3564 - val_mean_absolute_error: 2.2174\n",
      "\n",
      "Epoch 00058: val_loss improved from 7.52429 to 7.35641, saving model to best_model.h5\n",
      "Epoch 59/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 9.8547 - mean_absolute_error: 2.2030 - val_loss: 7.2079 - val_mean_absolute_error: 2.1657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00059: val_loss improved from 7.35641 to 7.20792, saving model to best_model.h5\n",
      "Epoch 60/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 9.7860 - mean_absolute_error: 2.1687 - val_loss: 7.4617 - val_mean_absolute_error: 2.2486\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 7.20792\n",
      "Epoch 61/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 9.7248 - mean_absolute_error: 2.1549 - val_loss: 7.2033 - val_mean_absolute_error: 2.2016\n",
      "\n",
      "Epoch 00061: val_loss improved from 7.20792 to 7.20332, saving model to best_model.h5\n",
      "Epoch 62/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 9.5873 - mean_absolute_error: 2.1676 - val_loss: 7.1560 - val_mean_absolute_error: 2.1892\n",
      "\n",
      "Epoch 00062: val_loss improved from 7.20332 to 7.15603, saving model to best_model.h5\n",
      "Epoch 63/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 9.5268 - mean_absolute_error: 2.1542 - val_loss: 7.1528 - val_mean_absolute_error: 2.1961\n",
      "\n",
      "Epoch 00063: val_loss improved from 7.15603 to 7.15282, saving model to best_model.h5\n",
      "Epoch 64/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 9.4289 - mean_absolute_error: 2.1444 - val_loss: 7.1791 - val_mean_absolute_error: 2.2049\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 7.15282\n",
      "Epoch 65/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 9.4841 - mean_absolute_error: 2.1612 - val_loss: 6.9812 - val_mean_absolute_error: 2.1780\n",
      "\n",
      "Epoch 00065: val_loss improved from 7.15282 to 6.98117, saving model to best_model.h5\n",
      "Epoch 66/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 9.3710 - mean_absolute_error: 2.1621 - val_loss: 6.8309 - val_mean_absolute_error: 2.1413\n",
      "\n",
      "Epoch 00066: val_loss improved from 6.98117 to 6.83095, saving model to best_model.h5\n",
      "Epoch 67/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 9.4612 - mean_absolute_error: 2.1762 - val_loss: 6.6400 - val_mean_absolute_error: 2.0528\n",
      "\n",
      "Epoch 00067: val_loss improved from 6.83095 to 6.63999, saving model to best_model.h5\n",
      "Epoch 68/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 9.4389 - mean_absolute_error: 2.1661 - val_loss: 6.6987 - val_mean_absolute_error: 2.0815\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 6.63999\n",
      "Epoch 69/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 9.2414 - mean_absolute_error: 2.1454 - val_loss: 7.1815 - val_mean_absolute_error: 2.1740\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 6.63999\n",
      "Epoch 70/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 9.2603 - mean_absolute_error: 2.1540 - val_loss: 6.8554 - val_mean_absolute_error: 2.1256\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 6.63999\n",
      "Epoch 71/3000\n",
      "355/355 [==============================] - 0s 101us/step - loss: 9.3224 - mean_absolute_error: 2.1490 - val_loss: 6.7326 - val_mean_absolute_error: 2.1206\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 6.63999\n",
      "Epoch 72/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 9.0954 - mean_absolute_error: 2.1237 - val_loss: 6.9310 - val_mean_absolute_error: 2.1458\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 6.63999\n",
      "Epoch 73/3000\n",
      "355/355 [==============================] - 0s 70us/step - loss: 9.1078 - mean_absolute_error: 2.1299 - val_loss: 6.6540 - val_mean_absolute_error: 2.0573\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 6.63999\n",
      "Epoch 74/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 9.1371 - mean_absolute_error: 2.1199 - val_loss: 6.5207 - val_mean_absolute_error: 2.0723\n",
      "\n",
      "Epoch 00074: val_loss improved from 6.63999 to 6.52071, saving model to best_model.h5\n",
      "Epoch 75/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 9.0328 - mean_absolute_error: 2.0973 - val_loss: 6.6727 - val_mean_absolute_error: 2.1393\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 6.52071\n",
      "Epoch 76/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 9.0081 - mean_absolute_error: 2.0950 - val_loss: 7.1197 - val_mean_absolute_error: 2.2327\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 6.52071\n",
      "Epoch 77/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 8.9506 - mean_absolute_error: 2.0962 - val_loss: 6.5892 - val_mean_absolute_error: 2.1106\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 6.52071\n",
      "Epoch 78/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 8.7616 - mean_absolute_error: 2.0731 - val_loss: 6.5817 - val_mean_absolute_error: 2.0810\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 6.52071\n",
      "Epoch 79/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 8.7480 - mean_absolute_error: 2.0830 - val_loss: 6.4878 - val_mean_absolute_error: 2.0587\n",
      "\n",
      "Epoch 00079: val_loss improved from 6.52071 to 6.48775, saving model to best_model.h5\n",
      "Epoch 80/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 8.7336 - mean_absolute_error: 2.0609 - val_loss: 6.5214 - val_mean_absolute_error: 2.0795\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 6.48775\n",
      "Epoch 81/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 8.7992 - mean_absolute_error: 2.0834 - val_loss: 6.8136 - val_mean_absolute_error: 2.1277\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 6.48775\n",
      "Epoch 82/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 8.6846 - mean_absolute_error: 2.0666 - val_loss: 6.6999 - val_mean_absolute_error: 2.1030\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 6.48775\n",
      "Epoch 83/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 8.5916 - mean_absolute_error: 2.0549 - val_loss: 6.5679 - val_mean_absolute_error: 2.0632\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 6.48775\n",
      "Epoch 84/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 8.9304 - mean_absolute_error: 2.1393 - val_loss: 6.8266 - val_mean_absolute_error: 2.1007\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 6.48775\n",
      "Epoch 85/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 8.6872 - mean_absolute_error: 2.0876 - val_loss: 6.7556 - val_mean_absolute_error: 2.0744\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 6.48775\n",
      "Epoch 86/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 8.5404 - mean_absolute_error: 2.0624 - val_loss: 6.6451 - val_mean_absolute_error: 2.0860\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 6.48775\n",
      "Epoch 87/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 8.4848 - mean_absolute_error: 2.0565 - val_loss: 6.5790 - val_mean_absolute_error: 2.0798\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 6.48775\n",
      "Epoch 88/3000\n",
      "355/355 [==============================] - 0s 70us/step - loss: 8.4849 - mean_absolute_error: 2.0454 - val_loss: 6.7879 - val_mean_absolute_error: 2.1013\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 6.48775\n",
      "Epoch 89/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 8.4331 - mean_absolute_error: 2.0252 - val_loss: 6.4730 - val_mean_absolute_error: 2.0306\n",
      "\n",
      "Epoch 00089: val_loss improved from 6.48775 to 6.47302, saving model to best_model.h5\n",
      "Epoch 90/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 8.3503 - mean_absolute_error: 2.0297 - val_loss: 6.4414 - val_mean_absolute_error: 2.0504\n",
      "\n",
      "Epoch 00090: val_loss improved from 6.47302 to 6.44142, saving model to best_model.h5\n",
      "Epoch 91/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 8.3304 - mean_absolute_error: 2.0382 - val_loss: 6.3819 - val_mean_absolute_error: 2.0442\n",
      "\n",
      "Epoch 00091: val_loss improved from 6.44142 to 6.38192, saving model to best_model.h5\n",
      "Epoch 92/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 8.2614 - mean_absolute_error: 2.0049 - val_loss: 6.4479 - val_mean_absolute_error: 2.0544\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 6.38192\n",
      "Epoch 93/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 8.2122 - mean_absolute_error: 2.0026 - val_loss: 6.5082 - val_mean_absolute_error: 2.0599\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 6.38192\n",
      "Epoch 94/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 8.1749 - mean_absolute_error: 1.9917 - val_loss: 6.4179 - val_mean_absolute_error: 2.0286\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 6.38192\n",
      "Epoch 95/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 73us/step - loss: 8.1183 - mean_absolute_error: 1.9942 - val_loss: 6.3657 - val_mean_absolute_error: 2.0168\n",
      "\n",
      "Epoch 00095: val_loss improved from 6.38192 to 6.36572, saving model to best_model.h5\n",
      "Epoch 96/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 8.1390 - mean_absolute_error: 1.9777 - val_loss: 6.5393 - val_mean_absolute_error: 2.0734\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 6.36572\n",
      "Epoch 97/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 8.1121 - mean_absolute_error: 1.9768 - val_loss: 6.3143 - val_mean_absolute_error: 2.0273\n",
      "\n",
      "Epoch 00097: val_loss improved from 6.36572 to 6.31430, saving model to best_model.h5\n",
      "Epoch 98/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 8.1830 - mean_absolute_error: 2.0459 - val_loss: 7.0677 - val_mean_absolute_error: 2.0656\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 6.31430\n",
      "Epoch 99/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 8.6244 - mean_absolute_error: 2.1201 - val_loss: 7.4115 - val_mean_absolute_error: 2.1613\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 6.31430\n",
      "Epoch 100/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 8.1792 - mean_absolute_error: 2.0104 - val_loss: 6.9657 - val_mean_absolute_error: 2.0414\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 6.31430\n",
      "Epoch 101/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 8.0684 - mean_absolute_error: 1.9830 - val_loss: 6.6918 - val_mean_absolute_error: 2.0582\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 6.31430\n",
      "Epoch 102/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.9322 - mean_absolute_error: 1.9718 - val_loss: 6.5527 - val_mean_absolute_error: 2.0442\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 6.31430\n",
      "Epoch 103/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 7.8692 - mean_absolute_error: 1.9710 - val_loss: 6.4501 - val_mean_absolute_error: 2.0375\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 6.31430\n",
      "Epoch 104/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.9234 - mean_absolute_error: 1.9736 - val_loss: 6.6460 - val_mean_absolute_error: 2.0979\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 6.31430\n",
      "Epoch 105/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 7.8507 - mean_absolute_error: 1.9626 - val_loss: 6.4348 - val_mean_absolute_error: 2.0533\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 6.31430\n",
      "Epoch 106/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 7.7719 - mean_absolute_error: 1.9655 - val_loss: 6.2909 - val_mean_absolute_error: 2.0230\n",
      "\n",
      "Epoch 00106: val_loss improved from 6.31430 to 6.29089, saving model to best_model.h5\n",
      "Epoch 107/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 7.8868 - mean_absolute_error: 1.9955 - val_loss: 6.3029 - val_mean_absolute_error: 2.0024\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 6.29089\n",
      "Epoch 108/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.8126 - mean_absolute_error: 1.9538 - val_loss: 6.5500 - val_mean_absolute_error: 2.0558\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 6.29089\n",
      "Epoch 109/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 7.7344 - mean_absolute_error: 1.9399 - val_loss: 6.0852 - val_mean_absolute_error: 1.9514\n",
      "\n",
      "Epoch 00109: val_loss improved from 6.29089 to 6.08517, saving model to best_model.h5\n",
      "Epoch 110/3000\n",
      "355/355 [==============================] - 0s 84us/step - loss: 7.7686 - mean_absolute_error: 1.9708 - val_loss: 6.1607 - val_mean_absolute_error: 1.9856\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 6.08517\n",
      "Epoch 111/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.7410 - mean_absolute_error: 1.9221 - val_loss: 6.4017 - val_mean_absolute_error: 2.0286\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 6.08517\n",
      "Epoch 112/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 7.6753 - mean_absolute_error: 1.9049 - val_loss: 6.2184 - val_mean_absolute_error: 2.0216\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 6.08517\n",
      "Epoch 113/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.7016 - mean_absolute_error: 1.9019 - val_loss: 6.1097 - val_mean_absolute_error: 1.9926\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 6.08517\n",
      "Epoch 114/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.5087 - mean_absolute_error: 1.8823 - val_loss: 6.0633 - val_mean_absolute_error: 1.9848\n",
      "\n",
      "Epoch 00114: val_loss improved from 6.08517 to 6.06332, saving model to best_model.h5\n",
      "Epoch 115/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.5606 - mean_absolute_error: 1.9268 - val_loss: 6.1175 - val_mean_absolute_error: 2.0010\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 6.06332\n",
      "Epoch 116/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.4206 - mean_absolute_error: 1.8936 - val_loss: 6.5201 - val_mean_absolute_error: 2.0514\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 6.06332\n",
      "Epoch 117/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 7.6319 - mean_absolute_error: 1.9389 - val_loss: 6.3286 - val_mean_absolute_error: 2.0298\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 6.06332\n",
      "Epoch 118/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 7.5634 - mean_absolute_error: 1.9148 - val_loss: 6.3597 - val_mean_absolute_error: 2.0607\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 6.06332\n",
      "Epoch 119/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 7.3951 - mean_absolute_error: 1.8943 - val_loss: 6.0337 - val_mean_absolute_error: 1.9694\n",
      "\n",
      "Epoch 00119: val_loss improved from 6.06332 to 6.03373, saving model to best_model.h5\n",
      "Epoch 120/3000\n",
      "355/355 [==============================] - 0s 84us/step - loss: 7.4710 - mean_absolute_error: 1.9342 - val_loss: 6.1028 - val_mean_absolute_error: 2.0034\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 6.03373\n",
      "Epoch 121/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 7.3307 - mean_absolute_error: 1.8862 - val_loss: 6.1916 - val_mean_absolute_error: 2.0028\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 6.03373\n",
      "Epoch 122/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.2562 - mean_absolute_error: 1.8576 - val_loss: 6.1236 - val_mean_absolute_error: 1.9790\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 6.03373\n",
      "Epoch 123/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 7.3172 - mean_absolute_error: 1.9061 - val_loss: 6.0714 - val_mean_absolute_error: 1.9808\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 6.03373\n",
      "Epoch 124/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 7.1444 - mean_absolute_error: 1.8610 - val_loss: 6.2814 - val_mean_absolute_error: 2.0201\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 6.03373\n",
      "Epoch 125/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 7.3090 - mean_absolute_error: 1.8532 - val_loss: 6.4235 - val_mean_absolute_error: 2.0449\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 6.03373\n",
      "Epoch 126/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 7.2247 - mean_absolute_error: 1.8351 - val_loss: 5.8976 - val_mean_absolute_error: 1.9479\n",
      "\n",
      "Epoch 00126: val_loss improved from 6.03373 to 5.89762, saving model to best_model.h5\n",
      "Epoch 127/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 7.2634 - mean_absolute_error: 1.8982 - val_loss: 5.9857 - val_mean_absolute_error: 1.9698\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 5.89762\n",
      "Epoch 128/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 7.2480 - mean_absolute_error: 1.8743 - val_loss: 6.4419 - val_mean_absolute_error: 2.0448\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 5.89762\n",
      "Epoch 129/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 7.2507 - mean_absolute_error: 1.8664 - val_loss: 6.1921 - val_mean_absolute_error: 1.9898\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 5.89762\n",
      "Epoch 130/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 7.0796 - mean_absolute_error: 1.8634 - val_loss: 6.1823 - val_mean_absolute_error: 2.0255\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 5.89762\n",
      "Epoch 131/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 76us/step - loss: 6.9755 - mean_absolute_error: 1.8411 - val_loss: 6.1220 - val_mean_absolute_error: 1.9875\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 5.89762\n",
      "Epoch 132/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 6.9860 - mean_absolute_error: 1.8396 - val_loss: 6.1658 - val_mean_absolute_error: 2.0048\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 5.89762\n",
      "Epoch 133/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.4136 - mean_absolute_error: 1.8871 - val_loss: 6.2305 - val_mean_absolute_error: 2.0202\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 5.89762\n",
      "Epoch 134/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.4103 - mean_absolute_error: 1.8686 - val_loss: 5.4827 - val_mean_absolute_error: 1.9143\n",
      "\n",
      "Epoch 00134: val_loss improved from 5.89762 to 5.48271, saving model to best_model.h5\n",
      "Epoch 135/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.1038 - mean_absolute_error: 1.8782 - val_loss: 5.8967 - val_mean_absolute_error: 2.0153\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 5.48271\n",
      "Epoch 136/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.8859 - mean_absolute_error: 1.8236 - val_loss: 5.9130 - val_mean_absolute_error: 1.9507\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 5.48271\n",
      "Epoch 137/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 6.9070 - mean_absolute_error: 1.8273 - val_loss: 5.7975 - val_mean_absolute_error: 1.9516\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 5.48271\n",
      "Epoch 138/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.7689 - mean_absolute_error: 1.8186 - val_loss: 5.8692 - val_mean_absolute_error: 1.9695\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 5.48271\n",
      "Epoch 139/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.7429 - mean_absolute_error: 1.8200 - val_loss: 6.0732 - val_mean_absolute_error: 1.9652\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 5.48271\n",
      "Epoch 140/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.9338 - mean_absolute_error: 1.8469 - val_loss: 5.8484 - val_mean_absolute_error: 1.9481\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 5.48271\n",
      "Epoch 141/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 6.8083 - mean_absolute_error: 1.8308 - val_loss: 5.8591 - val_mean_absolute_error: 1.9599\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 5.48271\n",
      "Epoch 142/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 6.8885 - mean_absolute_error: 1.7949 - val_loss: 6.0833 - val_mean_absolute_error: 2.0072\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 5.48271\n",
      "Epoch 143/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 6.8769 - mean_absolute_error: 1.8187 - val_loss: 5.8494 - val_mean_absolute_error: 1.9814\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 5.48271\n",
      "Epoch 144/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 6.7802 - mean_absolute_error: 1.8450 - val_loss: 5.7382 - val_mean_absolute_error: 1.9552\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 5.48271\n",
      "Epoch 145/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 6.6405 - mean_absolute_error: 1.8161 - val_loss: 5.9972 - val_mean_absolute_error: 1.9912\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 5.48271\n",
      "Epoch 146/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 6.5548 - mean_absolute_error: 1.7752 - val_loss: 5.9066 - val_mean_absolute_error: 1.9624\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 5.48271\n",
      "Epoch 147/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 6.6494 - mean_absolute_error: 1.8219 - val_loss: 5.9149 - val_mean_absolute_error: 1.9765\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 5.48271\n",
      "Epoch 148/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 6.4810 - mean_absolute_error: 1.7931 - val_loss: 6.0570 - val_mean_absolute_error: 1.9810\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 5.48271\n",
      "Epoch 149/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 6.4819 - mean_absolute_error: 1.7656 - val_loss: 5.7006 - val_mean_absolute_error: 1.9149\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 5.48271\n",
      "Epoch 150/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.3573 - mean_absolute_error: 1.7367 - val_loss: 5.6977 - val_mean_absolute_error: 1.9264\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 5.48271\n",
      "Epoch 151/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.3632 - mean_absolute_error: 1.7452 - val_loss: 5.6744 - val_mean_absolute_error: 1.9224\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 5.48271\n",
      "Epoch 152/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.3364 - mean_absolute_error: 1.7392 - val_loss: 5.7082 - val_mean_absolute_error: 1.9180\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 5.48271\n",
      "Epoch 153/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.2912 - mean_absolute_error: 1.7364 - val_loss: 5.7488 - val_mean_absolute_error: 1.9285\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 5.48271\n",
      "Epoch 154/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.3762 - mean_absolute_error: 1.7515 - val_loss: 5.6695 - val_mean_absolute_error: 1.9173\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 5.48271\n",
      "Epoch 155/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 6.3864 - mean_absolute_error: 1.7702 - val_loss: 5.5873 - val_mean_absolute_error: 1.9155\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 5.48271\n",
      "Epoch 156/3000\n",
      "355/355 [==============================] - 0s 85us/step - loss: 6.4811 - mean_absolute_error: 1.7787 - val_loss: 5.8750 - val_mean_absolute_error: 1.9297\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 5.48271\n",
      "Epoch 157/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 6.3790 - mean_absolute_error: 1.7681 - val_loss: 6.0767 - val_mean_absolute_error: 1.9870\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 5.48271\n",
      "Epoch 158/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.4198 - mean_absolute_error: 1.7766 - val_loss: 5.7206 - val_mean_absolute_error: 1.9044\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 5.48271\n",
      "Epoch 159/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 7.9101 - mean_absolute_error: 2.0077 - val_loss: 6.2156 - val_mean_absolute_error: 1.9691\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 5.48271\n",
      "Epoch 160/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 6.6221 - mean_absolute_error: 1.8426 - val_loss: 6.7174 - val_mean_absolute_error: 2.0947\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 5.48271\n",
      "Epoch 161/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.4562 - mean_absolute_error: 1.7896 - val_loss: 5.8702 - val_mean_absolute_error: 1.9593\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 5.48271\n",
      "Epoch 162/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.1712 - mean_absolute_error: 1.7452 - val_loss: 5.7307 - val_mean_absolute_error: 1.9264\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 5.48271\n",
      "Epoch 163/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 6.1721 - mean_absolute_error: 1.7456 - val_loss: 5.9411 - val_mean_absolute_error: 1.9607\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 5.48271\n",
      "Epoch 164/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 6.1509 - mean_absolute_error: 1.7720 - val_loss: 5.6728 - val_mean_absolute_error: 1.9266\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 5.48271\n",
      "Epoch 165/3000\n",
      "355/355 [==============================] - 0s 85us/step - loss: 6.0066 - mean_absolute_error: 1.7138 - val_loss: 5.6413 - val_mean_absolute_error: 1.8873\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 5.48271\n",
      "Epoch 166/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 6.1390 - mean_absolute_error: 1.7114 - val_loss: 5.5522 - val_mean_absolute_error: 1.9040\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 5.48271\n",
      "Epoch 167/3000\n",
      "355/355 [==============================] - 0s 85us/step - loss: 6.6449 - mean_absolute_error: 1.8592 - val_loss: 5.7529 - val_mean_absolute_error: 1.8935\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 5.48271\n",
      "Epoch 168/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 82us/step - loss: 6.2068 - mean_absolute_error: 1.7797 - val_loss: 6.2651 - val_mean_absolute_error: 1.9878\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 5.48271\n",
      "Epoch 169/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 6.2671 - mean_absolute_error: 1.8051 - val_loss: 5.9762 - val_mean_absolute_error: 1.9643\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 5.48271\n",
      "Epoch 170/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 6.0153 - mean_absolute_error: 1.7357 - val_loss: 5.5096 - val_mean_absolute_error: 1.9001\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 5.48271\n",
      "Epoch 171/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 5.8420 - mean_absolute_error: 1.6999 - val_loss: 5.6915 - val_mean_absolute_error: 1.9172\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 5.48271\n",
      "Epoch 172/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 5.8851 - mean_absolute_error: 1.6938 - val_loss: 5.6644 - val_mean_absolute_error: 1.9054\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 5.48271\n",
      "Epoch 173/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 5.8062 - mean_absolute_error: 1.6809 - val_loss: 5.4435 - val_mean_absolute_error: 1.8820\n",
      "\n",
      "Epoch 00173: val_loss improved from 5.48271 to 5.44351, saving model to best_model.h5\n",
      "Epoch 174/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 5.7123 - mean_absolute_error: 1.6709 - val_loss: 5.8208 - val_mean_absolute_error: 1.9260\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 5.44351\n",
      "Epoch 175/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 5.8847 - mean_absolute_error: 1.7141 - val_loss: 5.9318 - val_mean_absolute_error: 1.9607\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 5.44351\n",
      "Epoch 176/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.6353 - mean_absolute_error: 1.6748 - val_loss: 5.6152 - val_mean_absolute_error: 1.8989\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 5.44351\n",
      "Epoch 177/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 5.6544 - mean_absolute_error: 1.6588 - val_loss: 5.6748 - val_mean_absolute_error: 1.9132\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 5.44351\n",
      "Epoch 178/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 5.6574 - mean_absolute_error: 1.6805 - val_loss: 5.4536 - val_mean_absolute_error: 1.8710\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 5.44351\n",
      "Epoch 179/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 5.6380 - mean_absolute_error: 1.6689 - val_loss: 5.5287 - val_mean_absolute_error: 1.8837\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 5.44351\n",
      "Epoch 180/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 5.7977 - mean_absolute_error: 1.6696 - val_loss: 5.5685 - val_mean_absolute_error: 1.8905\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 5.44351\n",
      "Epoch 181/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.5797 - mean_absolute_error: 1.6560 - val_loss: 5.3598 - val_mean_absolute_error: 1.8639\n",
      "\n",
      "Epoch 00181: val_loss improved from 5.44351 to 5.35978, saving model to best_model.h5\n",
      "Epoch 182/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.5310 - mean_absolute_error: 1.6635 - val_loss: 5.5074 - val_mean_absolute_error: 1.8722\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 5.35978\n",
      "Epoch 183/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 5.6987 - mean_absolute_error: 1.6554 - val_loss: 5.4831 - val_mean_absolute_error: 1.8873\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 5.35978\n",
      "Epoch 184/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.6442 - mean_absolute_error: 1.6993 - val_loss: 5.4913 - val_mean_absolute_error: 1.8913\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 5.35978\n",
      "Epoch 185/3000\n",
      "355/355 [==============================] - 0s 85us/step - loss: 5.4487 - mean_absolute_error: 1.6209 - val_loss: 5.5054 - val_mean_absolute_error: 1.8794\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 5.35978\n",
      "Epoch 186/3000\n",
      "355/355 [==============================] - 0s 96us/step - loss: 5.5014 - mean_absolute_error: 1.6642 - val_loss: 5.3972 - val_mean_absolute_error: 1.8588\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 5.35978\n",
      "Epoch 187/3000\n",
      "355/355 [==============================] - 0s 104us/step - loss: 5.4901 - mean_absolute_error: 1.6575 - val_loss: 5.3418 - val_mean_absolute_error: 1.8345\n",
      "\n",
      "Epoch 00187: val_loss improved from 5.35978 to 5.34176, saving model to best_model.h5\n",
      "Epoch 188/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.4131 - mean_absolute_error: 1.6202 - val_loss: 5.4006 - val_mean_absolute_error: 1.8362\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 5.34176\n",
      "Epoch 189/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 5.3367 - mean_absolute_error: 1.6193 - val_loss: 5.2715 - val_mean_absolute_error: 1.8151\n",
      "\n",
      "Epoch 00189: val_loss improved from 5.34176 to 5.27148, saving model to best_model.h5\n",
      "Epoch 190/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.4289 - mean_absolute_error: 1.6062 - val_loss: 5.4244 - val_mean_absolute_error: 1.8468\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 5.27148\n",
      "Epoch 191/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.2299 - mean_absolute_error: 1.6034 - val_loss: 5.3103 - val_mean_absolute_error: 1.8451\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 5.27148\n",
      "Epoch 192/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 5.3159 - mean_absolute_error: 1.6155 - val_loss: 5.4748 - val_mean_absolute_error: 1.8626\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 5.27148\n",
      "Epoch 193/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.2953 - mean_absolute_error: 1.6071 - val_loss: 5.2819 - val_mean_absolute_error: 1.8207\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 5.27148\n",
      "Epoch 194/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.1075 - mean_absolute_error: 1.6013 - val_loss: 5.4526 - val_mean_absolute_error: 1.8530\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 5.27148\n",
      "Epoch 195/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.1592 - mean_absolute_error: 1.6088 - val_loss: 5.2893 - val_mean_absolute_error: 1.8231\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 5.27148\n",
      "Epoch 196/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.1538 - mean_absolute_error: 1.5778 - val_loss: 5.2226 - val_mean_absolute_error: 1.8194\n",
      "\n",
      "Epoch 00196: val_loss improved from 5.27148 to 5.22258, saving model to best_model.h5\n",
      "Epoch 197/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.0908 - mean_absolute_error: 1.5776 - val_loss: 5.4055 - val_mean_absolute_error: 1.8503\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 5.22258\n",
      "Epoch 198/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.0605 - mean_absolute_error: 1.5664 - val_loss: 5.2559 - val_mean_absolute_error: 1.8104\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 5.22258\n",
      "Epoch 199/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 5.0325 - mean_absolute_error: 1.5773 - val_loss: 5.3982 - val_mean_absolute_error: 1.8401\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 5.22258\n",
      "Epoch 200/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.0642 - mean_absolute_error: 1.5752 - val_loss: 5.3253 - val_mean_absolute_error: 1.8510\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 5.22258\n",
      "Epoch 201/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.0249 - mean_absolute_error: 1.5617 - val_loss: 5.3482 - val_mean_absolute_error: 1.8558\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 5.22258\n",
      "Epoch 202/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 5.1200 - mean_absolute_error: 1.5638 - val_loss: 5.2921 - val_mean_absolute_error: 1.8112\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 5.22258\n",
      "Epoch 203/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 5.2250 - mean_absolute_error: 1.6113 - val_loss: 5.1797 - val_mean_absolute_error: 1.8164\n",
      "\n",
      "Epoch 00203: val_loss improved from 5.22258 to 5.17968, saving model to best_model.h5\n",
      "Epoch 204/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 79us/step - loss: 4.9328 - mean_absolute_error: 1.5669 - val_loss: 5.3650 - val_mean_absolute_error: 1.8198\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 5.17968\n",
      "Epoch 205/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.1383 - mean_absolute_error: 1.5713 - val_loss: 5.4229 - val_mean_absolute_error: 1.8684\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 5.17968\n",
      "Epoch 206/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.9887 - mean_absolute_error: 1.5854 - val_loss: 5.3223 - val_mean_absolute_error: 1.8499\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 5.17968\n",
      "Epoch 207/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.8309 - mean_absolute_error: 1.5349 - val_loss: 5.3575 - val_mean_absolute_error: 1.8271\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 5.17968\n",
      "Epoch 208/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.8191 - mean_absolute_error: 1.5432 - val_loss: 5.1899 - val_mean_absolute_error: 1.8094\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 5.17968\n",
      "Epoch 209/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 5.1071 - mean_absolute_error: 1.5926 - val_loss: 5.2805 - val_mean_absolute_error: 1.8444\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 5.17968\n",
      "Epoch 210/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.9148 - mean_absolute_error: 1.5857 - val_loss: 5.5985 - val_mean_absolute_error: 1.8459\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 5.17968\n",
      "Epoch 211/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 5.0020 - mean_absolute_error: 1.5718 - val_loss: 5.2586 - val_mean_absolute_error: 1.7722\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 5.17968\n",
      "Epoch 212/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.9264 - mean_absolute_error: 1.5959 - val_loss: 5.0804 - val_mean_absolute_error: 1.7756\n",
      "\n",
      "Epoch 00212: val_loss improved from 5.17968 to 5.08042, saving model to best_model.h5\n",
      "Epoch 213/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 4.7328 - mean_absolute_error: 1.5042 - val_loss: 5.4768 - val_mean_absolute_error: 1.8597\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 5.08042\n",
      "Epoch 214/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 4.7144 - mean_absolute_error: 1.5174 - val_loss: 5.1365 - val_mean_absolute_error: 1.8201\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 5.08042\n",
      "Epoch 215/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 4.7132 - mean_absolute_error: 1.5035 - val_loss: 5.1871 - val_mean_absolute_error: 1.7999\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 5.08042\n",
      "Epoch 216/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 4.6391 - mean_absolute_error: 1.5242 - val_loss: 5.1538 - val_mean_absolute_error: 1.8032\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 5.08042\n",
      "Epoch 217/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 4.5742 - mean_absolute_error: 1.5006 - val_loss: 5.0950 - val_mean_absolute_error: 1.8026\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 5.08042\n",
      "Epoch 218/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 4.7286 - mean_absolute_error: 1.5157 - val_loss: 5.2084 - val_mean_absolute_error: 1.8149\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 5.08042\n",
      "Epoch 219/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.6130 - mean_absolute_error: 1.5037 - val_loss: 5.4748 - val_mean_absolute_error: 1.8587\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 5.08042\n",
      "Epoch 220/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.6100 - mean_absolute_error: 1.4848 - val_loss: 5.1457 - val_mean_absolute_error: 1.8155\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 5.08042\n",
      "Epoch 221/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 4.7715 - mean_absolute_error: 1.5372 - val_loss: 5.1611 - val_mean_absolute_error: 1.8114\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 5.08042\n",
      "Epoch 222/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.7808 - mean_absolute_error: 1.5129 - val_loss: 5.5525 - val_mean_absolute_error: 1.8553\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 5.08042\n",
      "Epoch 223/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 4.6536 - mean_absolute_error: 1.5320 - val_loss: 5.1646 - val_mean_absolute_error: 1.8129\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 5.08042\n",
      "Epoch 224/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 4.4333 - mean_absolute_error: 1.4663 - val_loss: 5.1853 - val_mean_absolute_error: 1.8073\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 5.08042\n",
      "Epoch 225/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 4.4569 - mean_absolute_error: 1.4762 - val_loss: 5.0681 - val_mean_absolute_error: 1.7905\n",
      "\n",
      "Epoch 00225: val_loss improved from 5.08042 to 5.06811, saving model to best_model.h5\n",
      "Epoch 226/3000\n",
      "355/355 [==============================] - 0s 85us/step - loss: 4.4342 - mean_absolute_error: 1.4560 - val_loss: 5.3225 - val_mean_absolute_error: 1.8432\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 5.06811\n",
      "Epoch 227/3000\n",
      "355/355 [==============================] - 0s 87us/step - loss: 4.3458 - mean_absolute_error: 1.4461 - val_loss: 5.3214 - val_mean_absolute_error: 1.8331\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 5.06811\n",
      "Epoch 228/3000\n",
      "355/355 [==============================] - 0s 87us/step - loss: 4.5135 - mean_absolute_error: 1.5151 - val_loss: 5.2289 - val_mean_absolute_error: 1.7763\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 5.06811\n",
      "Epoch 229/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 4.3321 - mean_absolute_error: 1.4411 - val_loss: 5.0393 - val_mean_absolute_error: 1.7763\n",
      "\n",
      "Epoch 00229: val_loss improved from 5.06811 to 5.03934, saving model to best_model.h5\n",
      "Epoch 230/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 4.2394 - mean_absolute_error: 1.4443 - val_loss: 5.0595 - val_mean_absolute_error: 1.7817\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 5.03934\n",
      "Epoch 231/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.2460 - mean_absolute_error: 1.4478 - val_loss: 5.1429 - val_mean_absolute_error: 1.7975\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 5.03934\n",
      "Epoch 232/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 4.2306 - mean_absolute_error: 1.4366 - val_loss: 5.1205 - val_mean_absolute_error: 1.8226\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 5.03934\n",
      "Epoch 233/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 4.1748 - mean_absolute_error: 1.4222 - val_loss: 5.1034 - val_mean_absolute_error: 1.8047\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 5.03934\n",
      "Epoch 234/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.1531 - mean_absolute_error: 1.4308 - val_loss: 5.2040 - val_mean_absolute_error: 1.8159\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 5.03934\n",
      "Epoch 235/3000\n",
      "355/355 [==============================] - 0s 85us/step - loss: 4.2917 - mean_absolute_error: 1.4520 - val_loss: 5.3002 - val_mean_absolute_error: 1.8471\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 5.03934\n",
      "Epoch 236/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 4.1913 - mean_absolute_error: 1.4188 - val_loss: 5.0718 - val_mean_absolute_error: 1.8054\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 5.03934\n",
      "Epoch 237/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 4.1024 - mean_absolute_error: 1.4208 - val_loss: 5.1237 - val_mean_absolute_error: 1.7895\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 5.03934\n",
      "Epoch 238/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.1117 - mean_absolute_error: 1.4030 - val_loss: 5.0147 - val_mean_absolute_error: 1.7529\n",
      "\n",
      "Epoch 00238: val_loss improved from 5.03934 to 5.01469, saving model to best_model.h5\n",
      "Epoch 239/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 4.1516 - mean_absolute_error: 1.4231 - val_loss: 5.0560 - val_mean_absolute_error: 1.7723\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 5.01469\n",
      "Epoch 240/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.0752 - mean_absolute_error: 1.4142 - val_loss: 5.1364 - val_mean_absolute_error: 1.7971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00240: val_loss did not improve from 5.01469\n",
      "Epoch 241/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 4.0910 - mean_absolute_error: 1.4141 - val_loss: 5.0466 - val_mean_absolute_error: 1.7655\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 5.01469\n",
      "Epoch 242/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 4.0196 - mean_absolute_error: 1.4013 - val_loss: 5.0665 - val_mean_absolute_error: 1.7664\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 5.01469\n",
      "Epoch 243/3000\n",
      "355/355 [==============================] - ETA: 0s - loss: 3.6934 - mean_absolute_error: 1.464 - 0s 76us/step - loss: 3.9684 - mean_absolute_error: 1.3836 - val_loss: 5.1572 - val_mean_absolute_error: 1.7914\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 5.01469\n",
      "Epoch 244/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 4.2081 - mean_absolute_error: 1.4661 - val_loss: 5.2741 - val_mean_absolute_error: 1.7686\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 5.01469\n",
      "Epoch 245/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 4.1287 - mean_absolute_error: 1.4466 - val_loss: 5.1158 - val_mean_absolute_error: 1.7043\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 5.01469\n",
      "Epoch 246/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 4.1108 - mean_absolute_error: 1.4145 - val_loss: 5.0223 - val_mean_absolute_error: 1.6954\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 5.01469\n",
      "Epoch 247/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.9934 - mean_absolute_error: 1.4259 - val_loss: 5.0072 - val_mean_absolute_error: 1.7467\n",
      "\n",
      "Epoch 00247: val_loss improved from 5.01469 to 5.00723, saving model to best_model.h5\n",
      "Epoch 248/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.9486 - mean_absolute_error: 1.3991 - val_loss: 5.0378 - val_mean_absolute_error: 1.7761\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 5.00723\n",
      "Epoch 249/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 4.2948 - mean_absolute_error: 1.4563 - val_loss: 5.2186 - val_mean_absolute_error: 1.8168\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 5.00723\n",
      "Epoch 250/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 4.2413 - mean_absolute_error: 1.5045 - val_loss: 5.7336 - val_mean_absolute_error: 1.8846\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 5.00723\n",
      "Epoch 251/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.8645 - mean_absolute_error: 1.3761 - val_loss: 5.2460 - val_mean_absolute_error: 1.8115\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 5.00723\n",
      "Epoch 252/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 3.8254 - mean_absolute_error: 1.3589 - val_loss: 5.2853 - val_mean_absolute_error: 1.7932\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 5.00723\n",
      "Epoch 253/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.8422 - mean_absolute_error: 1.3767 - val_loss: 5.1101 - val_mean_absolute_error: 1.7610\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 5.00723\n",
      "Epoch 254/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.7326 - mean_absolute_error: 1.3642 - val_loss: 5.2201 - val_mean_absolute_error: 1.7546\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 5.00723\n",
      "Epoch 255/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.7908 - mean_absolute_error: 1.3707 - val_loss: 5.0989 - val_mean_absolute_error: 1.7568\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 5.00723\n",
      "Epoch 256/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.7248 - mean_absolute_error: 1.3664 - val_loss: 5.0533 - val_mean_absolute_error: 1.7445\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 5.00723\n",
      "Epoch 257/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 3.7084 - mean_absolute_error: 1.3433 - val_loss: 5.1524 - val_mean_absolute_error: 1.7791\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 5.00723\n",
      "Epoch 258/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.6314 - mean_absolute_error: 1.3368 - val_loss: 5.1313 - val_mean_absolute_error: 1.7767\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 5.00723\n",
      "Epoch 259/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.7100 - mean_absolute_error: 1.3514 - val_loss: 5.1027 - val_mean_absolute_error: 1.7819\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 5.00723\n",
      "Epoch 260/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.6474 - mean_absolute_error: 1.3303 - val_loss: 5.1158 - val_mean_absolute_error: 1.7803\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 5.00723\n",
      "Epoch 261/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 3.5948 - mean_absolute_error: 1.3267 - val_loss: 5.1458 - val_mean_absolute_error: 1.7778\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 5.00723\n",
      "Epoch 262/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 3.6786 - mean_absolute_error: 1.3660 - val_loss: 5.0741 - val_mean_absolute_error: 1.7855\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 5.00723\n",
      "Epoch 263/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.6483 - mean_absolute_error: 1.3258 - val_loss: 4.9835 - val_mean_absolute_error: 1.7580\n",
      "\n",
      "Epoch 00263: val_loss improved from 5.00723 to 4.98349, saving model to best_model.h5\n",
      "Epoch 264/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.5894 - mean_absolute_error: 1.3158 - val_loss: 5.0568 - val_mean_absolute_error: 1.7838\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 4.98349\n",
      "Epoch 265/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.7201 - mean_absolute_error: 1.3972 - val_loss: 5.2010 - val_mean_absolute_error: 1.8226\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 4.98349\n",
      "Epoch 266/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.6093 - mean_absolute_error: 1.3334 - val_loss: 5.2094 - val_mean_absolute_error: 1.7877\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 4.98349\n",
      "Epoch 267/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 5.9501 - mean_absolute_error: 1.7542 - val_loss: 5.7433 - val_mean_absolute_error: 1.9174\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 4.98349\n",
      "Epoch 268/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 4.4170 - mean_absolute_error: 1.5613 - val_loss: 5.7675 - val_mean_absolute_error: 1.9214\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 4.98349\n",
      "Epoch 269/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.8851 - mean_absolute_error: 1.4414 - val_loss: 5.1920 - val_mean_absolute_error: 1.8020\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 4.98349\n",
      "Epoch 270/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.6320 - mean_absolute_error: 1.3402 - val_loss: 5.3636 - val_mean_absolute_error: 1.8551\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 4.98349\n",
      "Epoch 271/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.4896 - mean_absolute_error: 1.3236 - val_loss: 5.1102 - val_mean_absolute_error: 1.7899\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 4.98349\n",
      "Epoch 272/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 3.4829 - mean_absolute_error: 1.3139 - val_loss: 5.2382 - val_mean_absolute_error: 1.7856\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 4.98349\n",
      "Epoch 273/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.4590 - mean_absolute_error: 1.2977 - val_loss: 5.1453 - val_mean_absolute_error: 1.7717\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 4.98349\n",
      "Epoch 274/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 3.5509 - mean_absolute_error: 1.3707 - val_loss: 5.2146 - val_mean_absolute_error: 1.8001\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 4.98349\n",
      "Epoch 275/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.5166 - mean_absolute_error: 1.3220 - val_loss: 5.1225 - val_mean_absolute_error: 1.7841\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 4.98349\n",
      "Epoch 276/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.3995 - mean_absolute_error: 1.3045 - val_loss: 5.0963 - val_mean_absolute_error: 1.7890\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 4.98349\n",
      "Epoch 277/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 76us/step - loss: 3.8442 - mean_absolute_error: 1.3725 - val_loss: 5.2571 - val_mean_absolute_error: 1.8358\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 4.98349\n",
      "Epoch 278/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.4811 - mean_absolute_error: 1.3420 - val_loss: 5.2134 - val_mean_absolute_error: 1.7981\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 4.98349\n",
      "Epoch 279/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.7443 - mean_absolute_error: 1.3578 - val_loss: 5.0421 - val_mean_absolute_error: 1.7352\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 4.98349\n",
      "Epoch 280/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.3611 - mean_absolute_error: 1.3062 - val_loss: 5.1617 - val_mean_absolute_error: 1.8087\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 4.98349\n",
      "Epoch 281/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.3320 - mean_absolute_error: 1.2814 - val_loss: 5.0655 - val_mean_absolute_error: 1.7711\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 4.98349\n",
      "Epoch 282/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 3.2775 - mean_absolute_error: 1.2807 - val_loss: 5.2598 - val_mean_absolute_error: 1.8315\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 4.98349\n",
      "Epoch 283/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 3.2987 - mean_absolute_error: 1.2814 - val_loss: 5.1972 - val_mean_absolute_error: 1.8151\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 4.98349\n",
      "Epoch 284/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 3.4540 - mean_absolute_error: 1.3109 - val_loss: 5.3105 - val_mean_absolute_error: 1.8633\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 4.98349\n",
      "Epoch 285/3000\n",
      "355/355 [==============================] - 0s 70us/step - loss: 3.8603 - mean_absolute_error: 1.4351 - val_loss: 5.1306 - val_mean_absolute_error: 1.8119\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 4.98349\n",
      "Epoch 286/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.5933 - mean_absolute_error: 1.3534 - val_loss: 5.2067 - val_mean_absolute_error: 1.8416\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 4.98349\n",
      "Epoch 287/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.4648 - mean_absolute_error: 1.3455 - val_loss: 5.2244 - val_mean_absolute_error: 1.8315\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 4.98349\n",
      "Epoch 288/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 3.3478 - mean_absolute_error: 1.2895 - val_loss: 4.9881 - val_mean_absolute_error: 1.7656\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 4.98349\n",
      "Epoch 289/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.2542 - mean_absolute_error: 1.2686 - val_loss: 5.1931 - val_mean_absolute_error: 1.8004\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 4.98349\n",
      "Epoch 290/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 3.3592 - mean_absolute_error: 1.3322 - val_loss: 5.2555 - val_mean_absolute_error: 1.8294\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 4.98349\n",
      "Epoch 291/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 3.4555 - mean_absolute_error: 1.3524 - val_loss: 5.0749 - val_mean_absolute_error: 1.7714\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 4.98349\n",
      "Epoch 292/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.5650 - mean_absolute_error: 1.3358 - val_loss: 5.2740 - val_mean_absolute_error: 1.8406\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 4.98349\n",
      "Epoch 293/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 3.2691 - mean_absolute_error: 1.2695 - val_loss: 5.1209 - val_mean_absolute_error: 1.8004\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 4.98349\n",
      "Epoch 294/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.1856 - mean_absolute_error: 1.2811 - val_loss: 5.1363 - val_mean_absolute_error: 1.8102\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 4.98349\n",
      "Epoch 295/3000\n",
      "355/355 [==============================] - 0s 96us/step - loss: 3.1905 - mean_absolute_error: 1.2589 - val_loss: 5.2423 - val_mean_absolute_error: 1.8177\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 4.98349\n",
      "Epoch 296/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.2443 - mean_absolute_error: 1.2684 - val_loss: 5.1149 - val_mean_absolute_error: 1.7988\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 4.98349\n",
      "Epoch 297/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.5185 - mean_absolute_error: 1.3594 - val_loss: 5.1813 - val_mean_absolute_error: 1.8102\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 4.98349\n",
      "Epoch 298/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.2595 - mean_absolute_error: 1.3129 - val_loss: 5.0992 - val_mean_absolute_error: 1.8072\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 4.98349\n",
      "Epoch 299/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.2764 - mean_absolute_error: 1.2726 - val_loss: 5.1866 - val_mean_absolute_error: 1.8184\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 4.98349\n",
      "Epoch 300/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.2245 - mean_absolute_error: 1.2585 - val_loss: 5.4376 - val_mean_absolute_error: 1.8417\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 4.98349\n",
      "Epoch 301/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.4175 - mean_absolute_error: 1.3515 - val_loss: 5.3829 - val_mean_absolute_error: 1.8413\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 4.98349\n",
      "Epoch 302/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 3.3403 - mean_absolute_error: 1.2940 - val_loss: 5.1016 - val_mean_absolute_error: 1.7709\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 4.98349\n",
      "Epoch 303/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.1935 - mean_absolute_error: 1.3275 - val_loss: 5.3258 - val_mean_absolute_error: 1.8594\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 4.98349\n",
      "Epoch 304/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.1202 - mean_absolute_error: 1.2399 - val_loss: 5.2135 - val_mean_absolute_error: 1.8154\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 4.98349\n",
      "Epoch 305/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.9990 - mean_absolute_error: 1.2315 - val_loss: 5.3449 - val_mean_absolute_error: 1.8547\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 4.98349\n",
      "Epoch 306/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.9679 - mean_absolute_error: 1.2145 - val_loss: 5.2662 - val_mean_absolute_error: 1.8366\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 4.98349\n",
      "Epoch 307/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.9264 - mean_absolute_error: 1.2003 - val_loss: 5.1833 - val_mean_absolute_error: 1.8095\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 4.98349\n",
      "Epoch 308/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.9968 - mean_absolute_error: 1.2239 - val_loss: 5.2446 - val_mean_absolute_error: 1.8157\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 4.98349\n",
      "Epoch 309/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.9459 - mean_absolute_error: 1.2192 - val_loss: 5.3786 - val_mean_absolute_error: 1.8649\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 4.98349\n",
      "Epoch 310/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 3.0324 - mean_absolute_error: 1.2235 - val_loss: 5.2358 - val_mean_absolute_error: 1.8221\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 4.98349\n",
      "Epoch 311/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.0111 - mean_absolute_error: 1.2209 - val_loss: 5.2020 - val_mean_absolute_error: 1.7932\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 4.98349\n",
      "Epoch 312/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.9331 - mean_absolute_error: 1.2244 - val_loss: 5.1957 - val_mean_absolute_error: 1.7978\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 4.98349\n",
      "Epoch 313/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.9974 - mean_absolute_error: 1.2634 - val_loss: 5.2575 - val_mean_absolute_error: 1.8178\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 4.98349\n",
      "Epoch 314/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 76us/step - loss: 2.9957 - mean_absolute_error: 1.2550 - val_loss: 5.2383 - val_mean_absolute_error: 1.8093\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 4.98349\n",
      "Epoch 315/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.9036 - mean_absolute_error: 1.2342 - val_loss: 5.3088 - val_mean_absolute_error: 1.8406\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 4.98349\n",
      "Epoch 316/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.8901 - mean_absolute_error: 1.2142 - val_loss: 5.2973 - val_mean_absolute_error: 1.8438\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 4.98349\n",
      "Epoch 317/3000\n",
      "355/355 [==============================] - 0s 70us/step - loss: 2.9853 - mean_absolute_error: 1.2566 - val_loss: 5.4951 - val_mean_absolute_error: 1.8843\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 4.98349\n",
      "Epoch 318/3000\n",
      "355/355 [==============================] - 0s 70us/step - loss: 3.0227 - mean_absolute_error: 1.2345 - val_loss: 5.3262 - val_mean_absolute_error: 1.8118\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 4.98349\n",
      "Epoch 319/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.9188 - mean_absolute_error: 1.2334 - val_loss: 5.3577 - val_mean_absolute_error: 1.8308\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 4.98349\n",
      "Epoch 320/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.7793 - mean_absolute_error: 1.4082 - val_loss: 5.8445 - val_mean_absolute_error: 1.9390\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 4.98349\n",
      "Epoch 321/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.3141 - mean_absolute_error: 1.3436 - val_loss: 5.4016 - val_mean_absolute_error: 1.8282\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 4.98349\n",
      "Epoch 322/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 3.0532 - mean_absolute_error: 1.2641 - val_loss: 5.5488 - val_mean_absolute_error: 1.8568\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 4.98349\n",
      "Epoch 323/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.9565 - mean_absolute_error: 1.2237 - val_loss: 5.3637 - val_mean_absolute_error: 1.8373\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 4.98349\n",
      "Epoch 324/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.8307 - mean_absolute_error: 1.2035 - val_loss: 5.5423 - val_mean_absolute_error: 1.8665\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 4.98349\n",
      "Epoch 325/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.9025 - mean_absolute_error: 1.1925 - val_loss: 5.5251 - val_mean_absolute_error: 1.8617\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 4.98349\n",
      "Epoch 326/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.8413 - mean_absolute_error: 1.2050 - val_loss: 5.8679 - val_mean_absolute_error: 1.9447\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 4.98349\n",
      "Epoch 327/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 3.1318 - mean_absolute_error: 1.2671 - val_loss: 5.8036 - val_mean_absolute_error: 1.8891\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 4.98349\n",
      "Epoch 328/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.8259 - mean_absolute_error: 1.1947 - val_loss: 5.7662 - val_mean_absolute_error: 1.8967\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 4.98349\n",
      "Epoch 329/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.7806 - mean_absolute_error: 1.2011 - val_loss: 5.5700 - val_mean_absolute_error: 1.8571\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 4.98349\n",
      "Epoch 330/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.8013 - mean_absolute_error: 1.1857 - val_loss: 5.4327 - val_mean_absolute_error: 1.8357\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 4.98349\n",
      "Epoch 331/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.8512 - mean_absolute_error: 1.2468 - val_loss: 5.6560 - val_mean_absolute_error: 1.8975\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 4.98349\n",
      "Epoch 332/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.7027 - mean_absolute_error: 1.1652 - val_loss: 5.5592 - val_mean_absolute_error: 1.8472\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 4.98349\n",
      "Epoch 333/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.6687 - mean_absolute_error: 1.1679 - val_loss: 5.7342 - val_mean_absolute_error: 1.9055\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 4.98349\n",
      "Epoch 334/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.6434 - mean_absolute_error: 1.1484 - val_loss: 5.4880 - val_mean_absolute_error: 1.8737\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 4.98349\n",
      "Epoch 335/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.7976 - mean_absolute_error: 1.1713 - val_loss: 5.2193 - val_mean_absolute_error: 1.7980\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 4.98349\n",
      "Epoch 336/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.7144 - mean_absolute_error: 1.1775 - val_loss: 5.4281 - val_mean_absolute_error: 1.8369\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 4.98349\n",
      "Epoch 337/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.6403 - mean_absolute_error: 1.1592 - val_loss: 5.5591 - val_mean_absolute_error: 1.8715\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 4.98349\n",
      "Epoch 338/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.6487 - mean_absolute_error: 1.1492 - val_loss: 5.5988 - val_mean_absolute_error: 1.8700\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 4.98349\n",
      "Epoch 339/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.5830 - mean_absolute_error: 1.1496 - val_loss: 5.4870 - val_mean_absolute_error: 1.8268\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 4.98349\n",
      "Epoch 340/3000\n",
      "355/355 [==============================] - 0s 85us/step - loss: 2.5716 - mean_absolute_error: 1.1453 - val_loss: 5.5431 - val_mean_absolute_error: 1.8496\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 4.98349\n",
      "Epoch 341/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.6060 - mean_absolute_error: 1.1455 - val_loss: 5.5092 - val_mean_absolute_error: 1.8507\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 4.98349\n",
      "Epoch 342/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.6853 - mean_absolute_error: 1.1598 - val_loss: 5.5864 - val_mean_absolute_error: 1.8747\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 4.98349\n",
      "Epoch 343/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.8733 - mean_absolute_error: 1.2231 - val_loss: 6.0478 - val_mean_absolute_error: 2.0006\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 4.98349\n",
      "Epoch 344/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.6631 - mean_absolute_error: 1.1598 - val_loss: 5.7933 - val_mean_absolute_error: 1.8786\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 4.98349\n",
      "Epoch 345/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.5484 - mean_absolute_error: 1.1378 - val_loss: 5.6852 - val_mean_absolute_error: 1.9060\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 4.98349\n",
      "Epoch 346/3000\n",
      "355/355 [==============================] - 0s 99us/step - loss: 2.6986 - mean_absolute_error: 1.1471 - val_loss: 5.5146 - val_mean_absolute_error: 1.8203\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 4.98349\n",
      "Epoch 347/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.5905 - mean_absolute_error: 1.1706 - val_loss: 5.6614 - val_mean_absolute_error: 1.8708\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 4.98349\n",
      "Epoch 348/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.6737 - mean_absolute_error: 1.1553 - val_loss: 5.6947 - val_mean_absolute_error: 1.8604\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 4.98349\n",
      "Epoch 349/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.5441 - mean_absolute_error: 1.1340 - val_loss: 5.6032 - val_mean_absolute_error: 1.8535\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 4.98349\n",
      "Epoch 350/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.5304 - mean_absolute_error: 1.1261 - val_loss: 5.5147 - val_mean_absolute_error: 1.8325\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 4.98349\n",
      "Epoch 351/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 79us/step - loss: 2.6345 - mean_absolute_error: 1.1811 - val_loss: 5.8808 - val_mean_absolute_error: 1.9717\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 4.98349\n",
      "Epoch 352/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.5957 - mean_absolute_error: 1.1297 - val_loss: 5.6238 - val_mean_absolute_error: 1.8507\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 4.98349\n",
      "Epoch 353/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.5309 - mean_absolute_error: 1.1427 - val_loss: 5.6798 - val_mean_absolute_error: 1.8947\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 4.98349\n",
      "Epoch 354/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.5090 - mean_absolute_error: 1.1241 - val_loss: 5.6836 - val_mean_absolute_error: 1.8799\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 4.98349\n",
      "Epoch 355/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.5009 - mean_absolute_error: 1.1268 - val_loss: 5.7590 - val_mean_absolute_error: 1.8889\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 4.98349\n",
      "Epoch 356/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.5167 - mean_absolute_error: 1.1105 - val_loss: 5.6959 - val_mean_absolute_error: 1.8662\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 4.98349\n",
      "Epoch 357/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.4742 - mean_absolute_error: 1.1051 - val_loss: 5.6247 - val_mean_absolute_error: 1.8373\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 4.98349\n",
      "Epoch 358/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.5302 - mean_absolute_error: 1.1369 - val_loss: 5.8371 - val_mean_absolute_error: 1.9047\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 4.98349\n",
      "Epoch 359/3000\n",
      "355/355 [==============================] - 0s 85us/step - loss: 2.4745 - mean_absolute_error: 1.1129 - val_loss: 5.7669 - val_mean_absolute_error: 1.8982\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 4.98349\n",
      "Epoch 360/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.5051 - mean_absolute_error: 1.1131 - val_loss: 5.4486 - val_mean_absolute_error: 1.8250\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 4.98349\n",
      "Epoch 361/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.4811 - mean_absolute_error: 1.1078 - val_loss: 5.6621 - val_mean_absolute_error: 1.8726\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 4.98349\n",
      "Epoch 362/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.5853 - mean_absolute_error: 1.1212 - val_loss: 5.8308 - val_mean_absolute_error: 1.8673\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 4.98349\n",
      "Epoch 363/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.6495 - mean_absolute_error: 1.1732 - val_loss: 5.8325 - val_mean_absolute_error: 1.9151\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 4.98349\n",
      "Epoch 364/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.5339 - mean_absolute_error: 1.1469 - val_loss: 5.7271 - val_mean_absolute_error: 1.8703\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 4.98349\n",
      "Epoch 365/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.4595 - mean_absolute_error: 1.1027 - val_loss: 5.7609 - val_mean_absolute_error: 1.9068\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 4.98349\n",
      "Epoch 366/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.5112 - mean_absolute_error: 1.1308 - val_loss: 5.7671 - val_mean_absolute_error: 1.9149\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 4.98349\n",
      "Epoch 367/3000\n",
      "355/355 [==============================] - 0s 87us/step - loss: 2.4601 - mean_absolute_error: 1.1214 - val_loss: 5.7337 - val_mean_absolute_error: 1.8872\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 4.98349\n",
      "Epoch 368/3000\n",
      "355/355 [==============================] - 0s 85us/step - loss: 2.4525 - mean_absolute_error: 1.0914 - val_loss: 5.7262 - val_mean_absolute_error: 1.8445\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 4.98349\n",
      "Epoch 369/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.5766 - mean_absolute_error: 1.1594 - val_loss: 5.7360 - val_mean_absolute_error: 1.8951\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 4.98349\n",
      "Epoch 370/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.5293 - mean_absolute_error: 1.1403 - val_loss: 5.6551 - val_mean_absolute_error: 1.8477\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 4.98349\n",
      "Epoch 371/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.3739 - mean_absolute_error: 1.0905 - val_loss: 5.6552 - val_mean_absolute_error: 1.8893\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 4.98349\n",
      "Epoch 372/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.3777 - mean_absolute_error: 1.0730 - val_loss: 5.6908 - val_mean_absolute_error: 1.8425\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 4.98349\n",
      "Epoch 373/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.4826 - mean_absolute_error: 1.1161 - val_loss: 5.6114 - val_mean_absolute_error: 1.8740\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 4.98349\n",
      "Epoch 374/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.3498 - mean_absolute_error: 1.0676 - val_loss: 5.6923 - val_mean_absolute_error: 1.8557\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 4.98349\n",
      "Epoch 375/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.3069 - mean_absolute_error: 1.0707 - val_loss: 5.6398 - val_mean_absolute_error: 1.8359\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 4.98349\n",
      "Epoch 376/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.3005 - mean_absolute_error: 1.0562 - val_loss: 5.7173 - val_mean_absolute_error: 1.8465\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 4.98349\n",
      "Epoch 377/3000\n",
      "355/355 [==============================] - 0s 70us/step - loss: 2.2852 - mean_absolute_error: 1.0627 - val_loss: 5.7930 - val_mean_absolute_error: 1.8773\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 4.98349\n",
      "Epoch 378/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.2776 - mean_absolute_error: 1.0577 - val_loss: 5.6870 - val_mean_absolute_error: 1.8856\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 4.98349\n",
      "Epoch 379/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.8534 - mean_absolute_error: 1.2232 - val_loss: 6.0926 - val_mean_absolute_error: 1.9618\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 4.98349\n",
      "Epoch 380/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.7311 - mean_absolute_error: 1.2360 - val_loss: 5.8493 - val_mean_absolute_error: 1.9513\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 4.98349\n",
      "Epoch 381/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 3.0253 - mean_absolute_error: 1.2730 - val_loss: 6.1396 - val_mean_absolute_error: 1.9119\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 4.98349\n",
      "Epoch 382/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.5449 - mean_absolute_error: 1.1778 - val_loss: 6.3275 - val_mean_absolute_error: 1.9765\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 4.98349\n",
      "Epoch 383/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.4002 - mean_absolute_error: 1.0934 - val_loss: 5.9826 - val_mean_absolute_error: 1.9195\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 4.98349\n",
      "Epoch 384/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.9878 - mean_absolute_error: 1.2376 - val_loss: 6.5052 - val_mean_absolute_error: 2.0106\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 4.98349\n",
      "Epoch 385/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.6690 - mean_absolute_error: 1.2020 - val_loss: 5.9065 - val_mean_absolute_error: 1.8589\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 4.98349\n",
      "Epoch 386/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.4034 - mean_absolute_error: 1.0895 - val_loss: 5.9145 - val_mean_absolute_error: 1.8642\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 4.98349\n",
      "Epoch 387/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.4912 - mean_absolute_error: 1.1656 - val_loss: 6.2613 - val_mean_absolute_error: 1.9807\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 4.98349\n",
      "Epoch 388/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 76us/step - loss: 2.7079 - mean_absolute_error: 1.2066 - val_loss: 6.0774 - val_mean_absolute_error: 1.8956\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 4.98349\n",
      "Epoch 389/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.5477 - mean_absolute_error: 1.1723 - val_loss: 6.3324 - val_mean_absolute_error: 1.9683\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 4.98349\n",
      "Epoch 390/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.3326 - mean_absolute_error: 1.1016 - val_loss: 5.8391 - val_mean_absolute_error: 1.8729\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 4.98349\n",
      "Epoch 391/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.4665 - mean_absolute_error: 1.1124 - val_loss: 5.9048 - val_mean_absolute_error: 1.8751\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 4.98349\n",
      "Epoch 392/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.2207 - mean_absolute_error: 1.0441 - val_loss: 5.9138 - val_mean_absolute_error: 1.9555\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 4.98349\n",
      "Epoch 393/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.3235 - mean_absolute_error: 1.0807 - val_loss: 6.2985 - val_mean_absolute_error: 1.9644\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 4.98349\n",
      "Epoch 394/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.3937 - mean_absolute_error: 1.1026 - val_loss: 5.9546 - val_mean_absolute_error: 1.9079\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 4.98349\n",
      "Epoch 395/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.3334 - mean_absolute_error: 1.0769 - val_loss: 5.9069 - val_mean_absolute_error: 1.8690\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 4.98349\n",
      "Epoch 396/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.2184 - mean_absolute_error: 1.0740 - val_loss: 5.9310 - val_mean_absolute_error: 1.8993\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 4.98349\n",
      "Epoch 397/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.2670 - mean_absolute_error: 1.0749 - val_loss: 6.1258 - val_mean_absolute_error: 2.0101\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 4.98349\n",
      "Epoch 398/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.4345 - mean_absolute_error: 1.0889 - val_loss: 5.8452 - val_mean_absolute_error: 1.8467\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 4.98349\n",
      "Epoch 399/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.3211 - mean_absolute_error: 1.1002 - val_loss: 5.8607 - val_mean_absolute_error: 1.9275\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 4.98349\n",
      "Epoch 400/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.1754 - mean_absolute_error: 1.0334 - val_loss: 6.1022 - val_mean_absolute_error: 1.8839\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 4.98349\n",
      "Epoch 401/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.2209 - mean_absolute_error: 1.0445 - val_loss: 6.0860 - val_mean_absolute_error: 1.9089\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 4.98349\n",
      "Epoch 402/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.1289 - mean_absolute_error: 1.0352 - val_loss: 6.0565 - val_mean_absolute_error: 1.9436\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 4.98349\n",
      "Epoch 403/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.1040 - mean_absolute_error: 1.0028 - val_loss: 5.9274 - val_mean_absolute_error: 1.8969\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 4.98349\n",
      "Epoch 404/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.1344 - mean_absolute_error: 1.0188 - val_loss: 5.9809 - val_mean_absolute_error: 1.8986\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 4.98349\n",
      "Epoch 405/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.2164 - mean_absolute_error: 1.0570 - val_loss: 5.9350 - val_mean_absolute_error: 1.9129\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 4.98349\n",
      "Epoch 406/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.1875 - mean_absolute_error: 1.0501 - val_loss: 5.9476 - val_mean_absolute_error: 1.9184\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 4.98349\n",
      "Epoch 407/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.2333 - mean_absolute_error: 1.0729 - val_loss: 5.9044 - val_mean_absolute_error: 1.8628\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 4.98349\n",
      "Epoch 408/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.1246 - mean_absolute_error: 1.0402 - val_loss: 6.2601 - val_mean_absolute_error: 1.9820\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 4.98349\n",
      "Epoch 409/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.2022 - mean_absolute_error: 1.0521 - val_loss: 6.2392 - val_mean_absolute_error: 1.9293\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 4.98349\n",
      "Epoch 410/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.3042 - mean_absolute_error: 1.0698 - val_loss: 5.8467 - val_mean_absolute_error: 1.8890\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 4.98349\n",
      "Epoch 411/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.2323 - mean_absolute_error: 1.0691 - val_loss: 6.0274 - val_mean_absolute_error: 1.9461\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 4.98349\n",
      "Epoch 412/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.0803 - mean_absolute_error: 0.9977 - val_loss: 5.9618 - val_mean_absolute_error: 1.9181\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 4.98349\n",
      "Epoch 413/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.1513 - mean_absolute_error: 1.0246 - val_loss: 6.2304 - val_mean_absolute_error: 1.9196\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 4.98349\n",
      "Epoch 414/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.1071 - mean_absolute_error: 1.0111 - val_loss: 5.9907 - val_mean_absolute_error: 1.9326\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 4.98349\n",
      "Epoch 415/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.0783 - mean_absolute_error: 1.0060 - val_loss: 6.0488 - val_mean_absolute_error: 1.9063\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 4.98349\n",
      "Epoch 416/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.2097 - mean_absolute_error: 1.0525 - val_loss: 6.2433 - val_mean_absolute_error: 1.9797\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 4.98349\n",
      "Epoch 417/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.0661 - mean_absolute_error: 0.9971 - val_loss: 6.1091 - val_mean_absolute_error: 1.8849\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 4.98349\n",
      "Epoch 418/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.0805 - mean_absolute_error: 1.0122 - val_loss: 6.0647 - val_mean_absolute_error: 1.9148\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 4.98349\n",
      "Epoch 419/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.0357 - mean_absolute_error: 1.0103 - val_loss: 6.1735 - val_mean_absolute_error: 1.9369\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 4.98349\n",
      "Epoch 420/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.0361 - mean_absolute_error: 0.9925 - val_loss: 6.2358 - val_mean_absolute_error: 1.9256\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 4.98349\n",
      "Epoch 421/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.0671 - mean_absolute_error: 1.0304 - val_loss: 6.0769 - val_mean_absolute_error: 1.9515\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 4.98349\n",
      "Epoch 422/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.1169 - mean_absolute_error: 1.0041 - val_loss: 6.1367 - val_mean_absolute_error: 1.9275\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 4.98349\n",
      "Epoch 423/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.0248 - mean_absolute_error: 0.9917 - val_loss: 5.9995 - val_mean_absolute_error: 1.9608\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 4.98349\n",
      "Epoch 424/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 1.9930 - mean_absolute_error: 0.9786 - val_loss: 5.9197 - val_mean_absolute_error: 1.8595\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 4.98349\n",
      "Epoch 425/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 79us/step - loss: 2.1716 - mean_absolute_error: 1.0619 - val_loss: 6.3484 - val_mean_absolute_error: 1.9926\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 4.98349\n",
      "Epoch 426/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.1576 - mean_absolute_error: 1.0564 - val_loss: 6.3069 - val_mean_absolute_error: 1.9750\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 4.98349\n",
      "Epoch 427/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.0663 - mean_absolute_error: 1.0131 - val_loss: 6.0203 - val_mean_absolute_error: 1.8790\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 4.98349\n",
      "Epoch 428/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.1036 - mean_absolute_error: 1.0438 - val_loss: 6.4114 - val_mean_absolute_error: 2.0571\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 4.98349\n",
      "Epoch 429/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.0990 - mean_absolute_error: 1.0243 - val_loss: 6.0386 - val_mean_absolute_error: 1.9276\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 4.98349\n",
      "Epoch 430/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 2.0458 - mean_absolute_error: 1.0145 - val_loss: 5.9008 - val_mean_absolute_error: 1.8702\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 4.98349\n",
      "Epoch 431/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 1.9983 - mean_absolute_error: 0.9825 - val_loss: 5.9974 - val_mean_absolute_error: 1.8799\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 4.98349\n",
      "Epoch 432/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 1.9787 - mean_absolute_error: 0.9734 - val_loss: 5.9966 - val_mean_absolute_error: 1.8756\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 4.98349\n",
      "Epoch 433/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.0197 - mean_absolute_error: 0.9844 - val_loss: 6.0336 - val_mean_absolute_error: 1.9218\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 4.98349\n",
      "Epoch 434/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.0502 - mean_absolute_error: 1.0045 - val_loss: 6.0370 - val_mean_absolute_error: 1.8818\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 4.98349\n",
      "Epoch 435/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.0927 - mean_absolute_error: 1.0346 - val_loss: 5.9335 - val_mean_absolute_error: 1.9150\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 4.98349\n",
      "Epoch 436/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.0280 - mean_absolute_error: 1.0092 - val_loss: 6.5060 - val_mean_absolute_error: 2.0071\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 4.98349\n",
      "Epoch 437/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.0674 - mean_absolute_error: 1.0305 - val_loss: 6.2827 - val_mean_absolute_error: 1.9741\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 4.98349\n",
      "Epoch 438/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.1777 - mean_absolute_error: 1.0403 - val_loss: 6.1064 - val_mean_absolute_error: 1.8579\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 4.98349\n",
      "Epoch 439/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.1001 - mean_absolute_error: 1.0254 - val_loss: 5.9329 - val_mean_absolute_error: 1.9326\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 4.98349\n",
      "Epoch 440/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 1.9971 - mean_absolute_error: 0.9840 - val_loss: 6.1115 - val_mean_absolute_error: 1.9581\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 4.98349\n",
      "Epoch 441/3000\n",
      "355/355 [==============================] - 0s 82us/step - loss: 1.9989 - mean_absolute_error: 0.9853 - val_loss: 6.0615 - val_mean_absolute_error: 1.9098\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 4.98349\n",
      "Epoch 442/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 2.0320 - mean_absolute_error: 0.9776 - val_loss: 6.0454 - val_mean_absolute_error: 1.8914\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 4.98349\n",
      "Epoch 443/3000\n",
      "355/355 [==============================] - 0s 115us/step - loss: 2.0254 - mean_absolute_error: 1.0128 - val_loss: 6.3194 - val_mean_absolute_error: 1.9868\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 4.98349\n",
      "Epoch 444/3000\n",
      "355/355 [==============================] - 0s 124us/step - loss: 1.9455 - mean_absolute_error: 0.9574 - val_loss: 6.3566 - val_mean_absolute_error: 1.9458\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 4.98349\n",
      "Epoch 445/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.1288 - mean_absolute_error: 1.0241 - val_loss: 5.9392 - val_mean_absolute_error: 1.9084\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 4.98349\n",
      "Epoch 446/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.1218 - mean_absolute_error: 1.0509 - val_loss: 6.5961 - val_mean_absolute_error: 1.9935\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 4.98349\n",
      "Epoch 447/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.5968 - mean_absolute_error: 1.1596 - val_loss: 6.1006 - val_mean_absolute_error: 1.9454\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 4.98349\n",
      "Epoch 448/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.0866 - mean_absolute_error: 1.0374 - val_loss: 6.2748 - val_mean_absolute_error: 1.9457\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 4.98349\n",
      "Epoch 449/3000\n",
      "355/355 [==============================] - 0s 93us/step - loss: 2.0598 - mean_absolute_error: 1.0256 - val_loss: 6.1911 - val_mean_absolute_error: 2.0383\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 4.98349\n",
      "Epoch 450/3000\n",
      "355/355 [==============================] - 0s 96us/step - loss: 2.3789 - mean_absolute_error: 1.0845 - val_loss: 6.3734 - val_mean_absolute_error: 1.9326\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 4.98349\n",
      "Epoch 451/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.0384 - mean_absolute_error: 1.0227 - val_loss: 5.9757 - val_mean_absolute_error: 1.9318\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 4.98349\n",
      "Epoch 452/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.0504 - mean_absolute_error: 1.0093 - val_loss: 6.1306 - val_mean_absolute_error: 1.9192\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 4.98349\n",
      "Epoch 453/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 1.9532 - mean_absolute_error: 0.9793 - val_loss: 5.9072 - val_mean_absolute_error: 1.9162\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 4.98349\n",
      "Epoch 454/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 2.0818 - mean_absolute_error: 1.0189 - val_loss: 5.8103 - val_mean_absolute_error: 1.8098\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 4.98349\n",
      "Epoch 455/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.2964 - mean_absolute_error: 1.1048 - val_loss: 6.2684 - val_mean_absolute_error: 1.9426\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 4.98349\n",
      "Epoch 456/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 1.9172 - mean_absolute_error: 0.9870 - val_loss: 5.9617 - val_mean_absolute_error: 1.8552\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 4.98349\n",
      "Epoch 457/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 2.0205 - mean_absolute_error: 1.0041 - val_loss: 6.1139 - val_mean_absolute_error: 1.9428\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 4.98349\n",
      "Epoch 458/3000\n",
      "355/355 [==============================] - 0s 73us/step - loss: 1.9539 - mean_absolute_error: 0.9922 - val_loss: 6.0410 - val_mean_absolute_error: 1.9643\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 4.98349\n",
      "Epoch 459/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 1.8595 - mean_absolute_error: 0.9467 - val_loss: 5.8707 - val_mean_absolute_error: 1.8581\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 4.98349\n",
      "Epoch 460/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 1.8452 - mean_absolute_error: 0.9488 - val_loss: 5.9430 - val_mean_absolute_error: 1.9607\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 4.98349\n",
      "Epoch 461/3000\n",
      "355/355 [==============================] - 0s 76us/step - loss: 1.9945 - mean_absolute_error: 0.9937 - val_loss: 6.1222 - val_mean_absolute_error: 1.8974\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 4.98349\n",
      "Epoch 462/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 76us/step - loss: 1.9052 - mean_absolute_error: 0.9970 - val_loss: 6.4347 - val_mean_absolute_error: 2.0340\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 4.98349\n",
      "Epoch 463/3000\n",
      "355/355 [==============================] - 0s 79us/step - loss: 1.8557 - mean_absolute_error: 0.9473 - val_loss: 6.0828 - val_mean_absolute_error: 1.8455\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 4.98349\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, epochs=3000, verbose=1,\n",
    "                 validation_split=0.12,\n",
    "                 callbacks=[es, mc]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train','Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 196us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10.472205966126685, 2.198272873373593]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss / metric scores each\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
